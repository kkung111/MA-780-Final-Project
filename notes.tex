%\documentclass[twocolumn]{article}
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx,algpseudocode}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{bbm}
\usepackage{enumitem}   
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{chngpage}
\usepackage{breqn}
\usepackage{color,soul}
\usepackage{algorithm}
\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{ref.bib}
\linespread{1.3}

\usepackage[final]{changes}
\setdeletedmarkup{{\color{red}\sout{#1}}}
\usepackage{todonotes}
\newcommand{\kk}[2][]{\todo[color=purple!20,#1]{KK: #2}}

%\usepackage[backend=biber,style=authoryear]{biblatex}
%\bibliography{ref.bib}

\usepackage[margin=1.0in]{geometry}
\parskip = 0.1in


%%%%%%%%%%new commands%%%%%%%%%%%%
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\nsum}{{\sum_{i=1}^n}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\V}{{\text{Var}}}
\newcommand{\prob}{{\mathbb{P}}}
\newcommand{\bvar}[1]{\mathbf{#1}} 

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}

\newtheorem{Lemma}{Lemma}

\newtheorem{Def}{Definition}
\numberwithin{Def}{section}
\newtheorem{ex}{Example}
\newtheorem{Prop}{Proposition}
\newtheorem{Cor}[theorem]{Corollary}
\newtheorem{Rmk}{Remark}

%%%%%%%%%%%%%%%%%%%%%%

\title{Notes for U-Statistics}
\author{}
\date{\today}

\begin{document}
\section{Introduction}
\begin{itemize}
    \item $\theta = \theta(F)$ is defined as a functional, which is a mapping from a space $X$ into the reals. 
    \item We let $\mathcal{F}$ be the set of distribution function on $\mathbb{R}$
    \item We assume that a sample of random variables $X_1, \dotsc, X_n$ is i.i.d with d.f. $F \in \mathcal{F}\subset \R$. Note that $F$ is unknown but $\mathcal{F}$ is known. For example, $\mathcal{F}$ can be Exponential family? $\theta(F)$ is a functional defined on $\mathcal{F}$. 
    \item P.R. Halmos: 
    \begin{itemize}
        \item Does there exist an estimator of $\theta = \theta(F)$ that is unbiased, no matter what $F$ is. Can we characterize the sets $\mathcal{F}$ and $\theta$ for which the answer is yes.
        \item What is this estimator? What is the best estimator?
    \end{itemize}
    \item suppose that for each sufficiently large $n$, there is a function $f_n$ such that \begin{align}\label{unbiased}
        E(f_n(X_1, \dotsc, X_n)) = \theta(F).
    \end{align}
    Then, $\theta(F)$ is said to admit an unbiased estimator 
   \begin{theorem}
   (Halmos. Characterizing functionals)
    A functional $\theta$ defined on a set $\mathcal{F}$ of distribution functions admits an unbiased estimator if and only if there is a function $\psi$ of k variables such that 
    \begin{align}\label{unbiased_est}
        \theta(F) = \int_{-\infty}^{\infty} \dotsc \int_{-\infty}^{\infty} \psi(x_1, x_2, \dotsc, x_k)dF(x_1) \dotsc dF(x_k)
    \end{align}
    for all F in $\mathcal{F}$.
    \end{theorem}
    Here, $\theta$ is called a \textbf{regular statistical functional} of degree k and $\psi$ is called \textbf{the kernel} of the functional. 
    
    \item However, a more intuitive estimator is one that is based on a symmetric $\psi$ and incorporates all the data $X_1, \dotsc, X_n$ since the data is i.i.d. Here, symmetric means that it is invariant to permutations of the data, i.e. $f(x_1, x_2, x_3) = f(x_2, x_3, x_1)$. Thus, we consider functions $f_n$ that are symmetric and satisfies Equation \ref{unbiased}.
    
    \item With a sufficiently large $\mathcal{F}$, i.e. it contains all distributions with finite support in $E$, the Borel set, it turns out that there is a unique symmetric unbiased estimator, up to equality on $E$, i.e. they agree on some Borel set $E$. \kk{What exactly does this mean?} Note the choice of $E$ depends on $\mathcal{F}$. 
    
    \begin{theorem}\label{unique}
    Let $\mathcal{F}$ contain all distributions with finite support in $E$, and let $\theta$ be a regular functional satisfying Equation \ref{unbiased_est}. Then up to equality on $E$, there is a unique symmetric unbiased estimator of $\theta$. 
    \end{theorem}
    \begin{proof}
    Let $\psi^{[n]}(x_1, \dotsc, x_n) = \frac{(n-1)!}{n!}\sum \psi(x_{i_1}, \dotsc, x_{i_k})$ where the sum extends over all $\frac{(n-1)!}{n!}$ permutations $(i_1, \dotsc, i_k)$ of distinct integers chosen from $\{1, 2, \dotsc, n\}$. Then, $\psi^{[n]}(X_1, \dotsc, X_n)$ is unbiased since 
    \begin{align*}
        \int \dotsc \int_{\R_n} \psi^{[n]}(x_1, \dotsc, x_n)\prod_{i=1}^n dF(x_i) = \int \dotsc \int_{\R_*}\psi(x_1, \dotsc, x_k)\prod_{i=1}^k dF(x_i) = \theta(F).
    \end{align*}
    Now let $f$ be any other symmetric unbiased estimator. Then by applying Lemma \ref{lem A} to the function $f - \psi^{[n]}$, we see that $\psi^{[n]}$ is unique. 
    \end{proof}
    
    \begin{Lemma}\label{lem A}
    Let $\mathcal{F}$ contain all distributions with finite support in E, and let $f$ be a symmetric function of n variables with 
    \begin{align*}
        \int \dotsc \int_{\R_n} f(x_1, \dotsc, x_n) \prod_{i=1}^n dF(x_i) = 0 \text{ for all } F \in \mathcal{F}
    \end{align*}
    Then $f(x_1, \dotsc, x_n) = 0$ whenever $x_i \in E, i = 1, \dotsc, n$. 
    \end{Lemma}
    \begin{proof}
    For $i = 1, \dotsc, n$, let $x_i$ be a point in E, and let F be a distribution with points of increase $x_1, \dotsc, x_n$, and jumps $p_1, \dotsc, p_n$ at these points. Then 
    \begin{align*}
        \int \dotsc \int f(x_1, \dotsc, x_n) \prod_{i=1}^n dF(x_i) = \sum_{i_1 = 1}^n \dotsc \sum_{i_n = 1}^n f(x_{i_1}, \dotsc, x_{i_n}) p_{i_1}, \dotsc, p_{i_n} = 0
    \end{align*}
    and so the integral is a homogeneous polynomial in $p_1, \dotsc, p_n$ vanishing identically on the simplex $\sum p_i = 1, p_i \geq 0$. It follows that the polynomial vanishes identically; in particular so does the coefficients of $p_1, \dotsc, p_n$, which is given by $\sum f(x_{i_1}, \dotsc, x_{i_n})$ where the sum is taken over all permutations $(i_1, \dotsc, i_n)$ of $\{1, \dotsc, n\}$. But $f$ is symmetric in its arguments so that this implies that $f(x_1, \dotsc, x_n) = 0$. In the case when $E = \R$, the essentially unique symmetric estimate $\psi^{[n]}$ is also the one with minimum variance.
    \end{proof}
    
    \begin{theorem}
    Let $\theta$ be a regular functional of degree k defined by Equation \ref{unbiased_est} on a set of $\mathcal{F}$ of distribution functions containing all distributions having finite support. Let $f$ be an unbiased estimate of $\theta$ based on a sample of size n, so that $f$ satisfies Equation \ref{unbiased}. Then $var(f) \geq var(\psi^{[n]})$ for all $F \in \mathcal{F}$. 
    \end{theorem}
    \begin{proof}
    Define $f^{[n]}(x_1, \dotsc, x_n) = \frac{1}{n!}\sum_{(n)}f(x_{i_1}, \dotsc, x_{i_n})$ where here and in the sequel the sum $\sum_{(n)}$ is taken over all permutations $(i_1, \dotsc, i_n)$ of $\{1, \dotsc, n\}$. Then $f^{[n]}$ is a symmetric unbiased estimator, and so by Theorem \ref{unique} agrees with $\psi^{[n]}$ on $\R$. Hence by the Cauchy-Schwartz Inequality, 
    \begin{align*}
        (\psi^{[n]})^2 &= \left(\sum_{(n)}\frac{1}{n!}f(x_{i_1}, \dotsc, x_{i_n}) \right)^2 \\
        &\leq \sum_{(n)}\left(\frac{1}{n!}\right)^2\sum_{(n)} f^2(x_{i_1}, \dotsc, x_{i_n}) \\
        &=\frac{1}{n!} \sum_{(n)}f^2(x_{i_1}, \dotsc, x_{i_n})
    \end{align*}
    where the sums are taken over all permutations, and so 
    \begin{align*}
        E(\psi^{[n]})^2 &\leq \frac{1}{n!}\sum_{(n)}E(f^2(X_{i_1}, \dotsc, X_{i_n}))\\
        &= \frac{1}{n!}\sum_{(n)}E(f^2(X_1, \dotsc, X_n))\\
        &= E(f^2(X_1, \dotsc, X_n))\\
    \end{align*}
    which, since $E(\psi^{[n]}) = E(f) = \theta$, proves the result. This is because $var(X) = E(X^2) - E(X)^2$. Thus, we see that since $E(f^2(X_1, \dotsc, X_n)) \geq E(\psi^{[n]})^2$, we have that the variance of $f$ is greater. 
    \end{proof}
    
    \item The completeness of order statistics relative to a class $\mathcal{F}$ is equivalent to uniqueness of symmetric estimators unbiased for all $F \in \mathcal{F}$. This estimator will also have minimum variance. Note that given a different class of $\mathcal{F}$, the unbiased estimator may no longer be unique. 
    
    \item Define $\psi^{[k]}(x_1, \dotsc, x_n) = \frac{1}{k!}\sum \psi(x_{i_1}, \dotsc, x_{i_n})$ where the sum is taken over all permutations $(i_1, \dotsc, i_k)$ of $\{1, \dotsc, k\}$. Then, we can write 
    \begin{align}\label{ustat}
        \hat{\theta} = {n \choose k}^{-1} \sum_{(n,k)} \psi^{[k]}(X_{i_1}, \dotsc, X_{i_k})
    \end{align} 
    where the sum $\sum_{(n,k)}$ is taken over all subsets $1 \leq i_1< \dotsc <i_k \leq n$ of $\{1, \dotsc, n\}$. We will also use the notation $S_{n,k}$ to denote the set of $k$-subsets of $\{1, \dotsc, n\}$. Note that 
    \begin{align*}
        \int \dotsc \int_{\R_k}\psi(x_1, \dotsc, x_k) = \prod_{i=1}^k dF(x_i) = \int \dotsc \int_{\R_k} \phi^{[k]}(x_1, \dotsc, x_k) \prod_{i=1}^k dF(x_i)
    \end{align*}
    so that WLOG, we may take the functions $\phi$ defining regular functionals $\theta$ to be symmetric. The unique unbiased estimators are then of the form as in Equation \ref{ustat}.
\end{itemize}

\section{U-Statistics}
\begin{itemize}
    \item U-Statistics (Hoeffding, 1948), which stand for unbiased, are defined as 
        \begin{align*}
        \hat{\theta} = {n \choose k}^{-1} \sum_{(n,k)} \psi^{[k]}(X_{i_1}, \dotsc, X_{i_k})
    \end{align*} 
    \kk{What are regular functionals?}
    \item Examples include:
    \begin{itemize}
        \item Sample mean: $\mathcal{F}$ is set of all distributions whose means exist, i.e. it contains all distributions having finite support on $\R$. The functional is $\theta(F) = \int_{-\infty}^{\infty} x dF(x)$. The U-statistic if $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$
        
        \item Sample variance: $\mathcal{F}$ is set of all distributions with finite second moment, i.e. $\mathcal{F} =\{F: \int|x|^2 dF(x)<\infty\}$. The variance functional is given as $var(F) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2}(x_1 - x_2)^2dF(x_1)dF(x_2)$ which is estimated by $s_n^2 = {n \choose 2}^{-1}\sum_{1\leq i <j \leq n}\frac{1}{2}(X_i - X_j)^2$
    \end{itemize}
    \end{itemize}
    \subsection{Variance }
    \begin{itemize}

        \item For $c = 1, \dotsc, k$, let \begin{align}
            \psi_c(x_1, \dotsc, x_c) = E(\psi(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_k))
        \end{align}
        and variances $\sigma_c^2 = var(\psi_c(X_1, \dotsc, X_c))$. So the conditional expectation is dependent on the first $c$ random variables. Also define $\sigma_0^2 = 0$. Then, we have the following theorem.
        \begin{theorem}
        The functions $\psi_c$ defined above have the properties
        \begin{enumerate}[label = (\roman*)]
            \item $\psi_c(x_1, \dotsc, x_c) = E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)), \text{ for } 1 \leq c <d \leq k$
            \item $E(\psi_c(X_1, \dotsc, X_c)) = E(\psi(X_1, \dotsc, X_k))$
        \end{enumerate}
        \end{theorem}

    \begin{proof}
    \begin{enumerate}[label = (\roman*)]
        \item \begin{align*}
            E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)) &= \int \dotsc \int \psi_d(x_1, \dotsc, x_c, x_{c+1}, \dotsc x_d) \prod_{i=c+1}^d dF(x_i) \\
            &= \int \dotsc \int \left\{ \int \dotsc \int \psi(x_1, \dotsc, x_d, \dotsc x_k) \prod_{i=d+1}^k dF(x_i)\right\}\prod_{i=c+1}^d dF(x_i)\\
            &= \ \int \dotsc \int \psi(x_1, \dotsc, x_c, x_{c+1}, \dotsc x_d) \prod_{i=d+1}^d dF(x_i)\prod_{i=c+1}^d dF(x_i)\\
            &=\psi_c(x_1, \dotsc, x_c)
        \end{align*}
        
        \item \begin{align*}
            E(\psi_c(X_1, \dotsc, X_c)) &= \int \dotsc \int \phi_c(x_1, \dotsc, x_c) \prod_{i=1}^c dF(x_i) \\
            &= \int \dotsc \int  \left\{ \int \dotsc \int \psi(x_1, \dotsc, x_k) \prod_{i=c+1}^k dF(x_i)\right\}\prod_{i=1}^c dF(x_i) \\
            &= \int \dotsc \int psi(x_1, \dotsc, x_k) \prod_{i=1}^k dF(x_i)\\
            &=E(\psi(X_1, \dotsc, X_k))
        \end{align*}
    \end{enumerate}
    \end{proof}
    
    Note that there is an easier proof:
    \begin{proof}
    \begin{enumerate}[label = (\roman*)]
        \item We have that for $1 \leq c <d \leq k$
        \begin{align*}
            E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)) &= E(E(\psi(X_1, \dotsc, X_c, X_{c+1}, \dotsc, X_k)|x_1, \dotsc x_{d})|x_1, \dotsc, x_c)\\
            &= E(\psi(X_1, \dotsc, X_k)|x_1, \dotsc, x_c) &&\text{[since $c<d$]}\\
            &=\psi_c(x_1, \dotsc, x_c)
        \end{align*}
        
        \item 
        \begin{align*}
            E(\psi_c(X_1, \dotsc, X_c)) &= E(E(\psi(X_1, \dotsc, X_k)|x_1, \dotsc, x_c)) \\
            &=E(\psi(X_1, \dotsc, X_k))\\
        \end{align*}
    \end{enumerate}
    \end{proof}
    \item Another way to write $\sigma_c^2$ is 
    \begin{align*}
        \sigma_c^2 = cov(\psi(S_1), \psi(S_2))
    \end{align*}
    where $S_1, S_2$ are two k-subsets of $\{1, 2, \dotsc, n\}$ with c elements in common. 
    
    \begin{theorem}
    Let $U_n$ be a U-statistic with a kernel $\psi$ of degree k. Then, 
    \begin{align}
        var(U_n) = {n \choose k}^{-1}\sum_{c=1}^k{k \choose c}{n-k \choose k-c} \sigma_c^2
    \end{align}
    \end{theorem}
    \begin{proof}
    \begin{align*}
        var(U_n) &= var({n \choose k}^{-1} \sum_{(n,k)}\psi(X_{i_1}, \dotsc, X_{i_n}))\\
        &= cov({n \choose k}^{-1} \sum_{(n,k)}\psi(X_{i_1}, \dotsc, X_{i_n}), {n \choose k}^{-1} \sum_{(n,k)}\psi(X_{j_1}, \dotsc, X_{j_n}))\\
        &= {n \choose k}^{-2}\sum_{(n,k)}\sum_{(n,k)}cov(\psi(X_{i_1}, \dotsc, X_{i_n}), \psi(X_{j_1}, \dotsc, X_{j_n}))
    \end{align*}
    
    Consider the number of ways of choosing a pair of k subsets that have c elements in common. The first member of the pair can be chosen in ${n \choose k}$ ways. The c elements of the second set that are common with the first can be chosen in ${k \choose c}$ ways, and the $k-c$ elements distinct from these can be chosen in ${n-k \choose k-c}$ ways. Thus, the number of pairs having c elements in common is ${n choose k}{k \choose c}{n-k \choose k-c}$ and so we have \begin{align*}
        {n \choose k}^{-1}\sum_{c=1}^k {k \choose c}{n-k \choose k-c}\sigma_c^2
    \end{align*}
    \end{proof}
\end{itemize}

\subsection{Covariance}
\begin{itemize}
    \item Let $U_n^{(1)}$ and $U_n^{(2)}$ be two U-statistics, both based on a common sample $X_1, \dotsc, X_n$ but with different kernels $\psi$ and $\phi$ of degrees $k_1$ and $k_2$, respectively with $k_1 \leq j_2$. We define $\sigma_{c,d}^2$ as the covariance between the conditional expectations $\psi_c(X_1, \dotsc, X_c)$ and $\phi_d(X_1, \dotsc, X_d)$ and if $S$ is a set, let $|S|$ denote the size of $S$.
    
    \begin{theorem}
    Suppose that $c \leq d$. If $S_1$ is in $S_{n,k_1}$ and $S_2$ in $S_{n, k_2}$ with $|S_1 \cap S_2| = c$, then 
    \begin{align}
        \sigma_{c,d}^2 = cov(\psi(S_1), \phi(S_2))
    \end{align}
    \end{theorem}
    
    As a consequence, we have $\sigma_{c,c}^2 = \sigma_{c,c+1}^2 = \dotsc = \sigma_{c,k_2}^2$ for $c = 1, \dotsc, k_1$. 
    
    \begin{theorem}
    Let $U_n^{(1)}$ and $U_n^{(1)}$ be defined as above. Then, 
    \begin{align}
        cov(U_n^{(1)}, U_n^{(2)}) = {n \choose k_1}^{-1}\sum_{c=1}^{k_1}{k_2 \choose c}{n-k_1 \choose k_1 - c}\sigma_{c,c}^2
        \end{align}
    \end{theorem}
    \begin{proof}
    \begin{align*}
      cov(U_n^{(1)}, U_n^{(2)}) &= {n \choose k_1}^{-1}{n \choose k_2}^{-1}\sum_{(n,k_1)}\sum_{(n, k_2)} cov(\psi(S_1), \phi(S_2)) \\
      &= {n \choose k_1}^{-1}{n \choose k_2}^{-1}\sum_{c=1}^{k_1}\sum_{|S_1 \cap S_2| = c} cov(\psi(S_1), \phi(S_2)) \\
      &= {n \choose k_1}^{-1}\sum_{c=1}^{k_1}{k_2 \choose c}{n-k_1 \choose k_1 - c}\sigma_{c,c}^2
    \end{align*}
    \end{proof}
    
    We can also consider the covariance between U-statistics that are based on different number of observations. 
    \begin{theorem}
    Let $U_n$ and $U_m$ be U-statistics based on the same kernel $\psi$ of degree $k$ but on different numbers $n$ and $m$ of observations. Then if $m<n$, 
    \begin{align}
        cov(U_n, U_m) = {n \choose k}^{-1} \sum_{c=1}^k {k \choose c}{n-k \choose k-c}\sigma_c^2 = var(U_n)
    \end{align}
    \end{theorem}
    \begin{proof}
    Of the ${m \choose k}{n \choose k}$ terms in the sum $\sum_{(m,k)}\sum_{(n,k)}cov(\psi(S_1), \psi(S_2))$, exactly ${m \choose k}{k \choose c}{n-k \choose k-c}$ have c variables in common. 
    \end{proof}
\end{itemize}

\subsection{Higher Moments}
\begin{theorem}(Grams and Serfling 1973)
Suppose that $E(|\psi(X_1, \dotsc, X_k)|^r)<\infty$ where $r \geq 2$. Then, $E(|U_n - \theta|^r) = O(n^{-r/2})$
\end{theorem}

\begin{Lemma}
Let $X_1, \dotsc, X_n$ be a sequence of i.i.d distributed zero mean random variables satisfying $E(|X_n|^r)<\infty$ and let $S_n = X_1 + \dotsc + X_n$. If $r \geq 2$, then $E(|\frac{1}{n}S_n|^r) = O(n^{-r/2})$.
\end{Lemma}

\begin{theorem}
Suppose that $0 = \sigma_1 = \dotsc = \sigma_{d-1}^2<\sigma_d^2$. Let $r$ be an integer $\geq 2$. Then $E(U_n - \theta)^r = O(n^{[\frac{rd + 1}{2}]})$ where $[x]$ is the floor of x. 
\end{theorem}

\begin{Lemma}
Let $S_1, \dotsc, S_n$ be finite sets, and define $T_i = \cup_{j \neq i}S_j$. Denote the number of elements in a set $S$ by $|S|$. Then, $2|\cup_{i \neq j}(S_i \cap S_j)|\leq \sum_{i=1}^n|S_i \cap T_i|$. 
\end{Lemma}

\subsection{H-decomposition}
Here, we treat a representation of U-statistics of degree $k$ in terms of sums of uncorrelated U-statistics of degree $1, \dotsc, k$. The decomposition is by Hoeffding (1961), We first introduce kernels $h^{(1)}, \dotsc, h^{(k)}$ of degrees $1, \dotsc, k$ which are defined recursively by the equations 
\begin{align}
    h^{(1)}(x_1) &= \psi_1(x_1) - \theta \\
    h^{(c)}(x_1, \dotsc, x_c) &= \psi_c(x_1, \dotsc, x_c) - \sum_{j=1}^{c-1}\sum_{(c,j)}h^{(j)}(x_{i_1}, \dotsc, x_{i_j}) - \theta
\end{align}
for $c = 2, 3, \dotsc, k$. Let $S_j\{i_1, \dotsc, i_k\}$ denote the sum $\sum h^{(j)}(x_{\nu_1}, \dotsc x_{\nu_j})$ summed over all j-subsets $\{\nu_1, \dotsc, \nu_j\}$ of $\{i_1, \dotsc, i_k\}$. Then using the relationship
\begin{align}
    \sum_{(n,k)}S_j(i_1, \dotsc, i_k) &= {n-j \choose k-j} \sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j})
\end{align}
the identity ${n \choose k}^{-1}{n-j \choose k-j} = {k \choose j}{n \choose j}^{-1}$ and the recursive relationship above, we have
\begin{align*}
    U_n &= {n \choose k}^{-1}\sum_{(n,k)}\psi(x_{i_1}, \dotsc, x_{i_k})\\
    &= {n \choose k}^{-1}\sum_{(n,k)}(\sum_{j=1}^kS_j(i_1, \dotsc, i_k) + \theta)\\
    &= \theta + {n \choose k}^{-1}\sum_{j=1}^k{n-j \choose k-j}\sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc x_{\nu_j}) \\
    &=\theta + \sum_{j=1}^k{k \choose j}H_n^{(j)}
\end{align*}
where $H_n^{(j)}$ is the U-statistic of degree j based on kernel $h^{(j)}$. To be more formal, we have

\begin{theorem}
For $j = 1, \dotsc, k$, let $H_n^{(j)}$ be the U-statistic based on the kernel $h^{(j)}$ defined above. Then 
\begin{align}
    U_n  = \theta + \sum_{j=1}^k {k \choose j}H_n^{(j)}
\end{align}
\end{theorem}

\begin{itemize}
    \item This decomposition is useful because $H_n^{(j)}$ are uncorrelated, with variances of decreasing order in n. 
    \item Also, if we truncate the H-decomposition after c terms, we get 
    \begin{align*}
        U_n = \theta + \sum_{j=1}^c {k \choose j}H_n^{(j)} + R_n^{(c)}
    \end{align*}
    where $R_n^{(c)}$ is the remainder and is a U-statistic with kernel $\sum_{j=c+1}^k g_j$.
    
    \item Some properties of $h^{(j)}$ include
    \begin{theorem}
    \begin{itemize}
        \item For $c = 1, \dotsc, j-1$ and $j = 1, \dotsc, k$, $h_c^{(j)}(x_1, \dotsc, x_c) = 0$
        \item $E(h^{(j)}(X_1, \dotsc, X_j)) = 0$
    \end{itemize}
    \end{theorem}
    
    \item The variances of the conditional expectations of $h^{(j)}$ are all zero, except hte j-th. We define \begin{align}
        \delta_j^2 = var(h_j^{(j)}(X_1, \dotsc, X_j)) = var(h^{(j)}(X_1, \dotsc, X_j))
    \end{align}
    
    Then, we have that 
    \begin{align}
        var(H_n^{(j)}) = {n \choose j}^{-1}\sum_{c=1}^j {j \choose c}{n-j \choose j-c}var(h_c^{(j)}(X_1, \dotsc X_c)) = {n \choose j}^{-1}\delta_j^2
    \end{align}
    
    \begin{theorem}
    \begin{itemize}
        \item Let $j <j'$ and let $S_1$ and $S_2$ be a j-subset of $\{1, \dotsc, n\}$ and a j'-subset of $\{1, \dotsc, n\}$, respectively. Then, $cov(h^{(j)}(S_1), h^{(j')}(S_2)) = 0$ and so $cov(H_n^{(j)}, H_n^{(j')}) = 0$
        \item Let $S_1 \neq S_2$ be two distinct j-subsets of $\{1, \dotsc, n\}$. Then, $cov(h^{(j)}(S_1), h^{(j)}(S_2)) = 0$
    \end{itemize}
    \end{theorem}
    


    
    \begin{theorem}
    The variance of $U_n$ is given by 
    \begin{align}
        var(U_n) = \sum_{j=1}^k {k \choose j}^2 {n \choose j}^{-1}\delta_j^2
    \end{align}
    \end{theorem}
    \begin{proof}
    \begin{align*}
        var(U_n) &= var(\theta + \sum_{j=1}^k {k \choose j}H_n^{(j)} \\
        &= \sum_{j=1}^k {k \choose j}^2var(H_n^{(j)})\\
        &= \sum_{j=1}^k {k \choose j}^2 {n \choose j}^{-1}\delta_j^2
    \end{align*}
    \end{proof}
    \end{itemize}
    Relations describing $\sigma_{c}^2$ in terms of the $\delta_j^2 $ and vice versa can be derived as follows: from the definition of H-decomposition, we have 
    \begin{align*}
        \phi_c (X_1, \cdots, X_c) = \sum_{j = 1}^c \sum_{c,j} h^{(j)} (X_{i_{1}}, \cdots , X_{i_j}) + \theta
    \end{align*}
    thus take variance of both sides, we get 
    \begin{align*}
        \sigma_{c}^2 &= \sum_{j = 1}^c Var\{ \sum_{(c, j)} h^{(j)} (X_{i_{1}}, \cdots , X_{i_j})\} \\
        &= \sum_{j = 1}^c {c \choose j}^2 Var( H_c^{(j)}) \\ 
        &= \sum_{j = 1}^c {c \choose j} \delta_j^2.
    \end{align*}
    
    By using binomial coefficients, we could try to invert the last expression. Multiply both sides of the equation above by  $(-1)^{d-c} {d \choose c}$, and taking the sum from $c = 1$ to $d$ gives us
    \begin{align*}
        \sum_{c=1}^d {d \choose c} (-1)^{d-c} \sigma_{c}^2 &= \sum_{c=1}^d  
       \sum_{j = 1}^c {c \choose j}  {d \choose c} (-1)^{d-c} \delta_j^2 \\
       &= \sum_{j=1}^d \left\{ \sum_{c = 0}^{d - j} {d - j \choose c} (-1)^c \right\} (-1)^{d -j} {d \choose j} \delta_j^2.
    \end{align*}
    The term inside the braces is zero except when $j = d$; hence we get 
    \begin{align*}
        \delta_d^2 = \sum_{c = 1}^d (-1)^{b-c} {d \choose c} \sigma_{c}^2
    \end{align*}
    The reason we care about $\delta_d^2$ is because if a U-statistics is degenerate of order $d$, i.e., if $0 = \sigma_1^2 = \cdots \sigma_d^2 < \sigma_{d+1}^2$, then the first $d$ terms of the H-decomposition vanish, since in this case $\delta_1^2 = \cdots = \delta_d^2 = 0.$ When we discuss asymptotic of U-statistics later on, we'd see that the order of degeneracy determines the asymptotic distribution of the U-statistics.  
    
    
    
    \section{Asymptotics} 
    \begin{theorem}
    Let $\sigma_1^{2} > 0$. Then $n^{\frac{1}{2}}(U_n - \theta)$ is asymptotically normal with mean zero and asymptotic variance $k^2 \sigma_1^2$.
    \end{theorem}
    
    \begin{proof}
    
    \end{proof}
    
    \begin{theorem}[we may not need this]
    Let $U_n^{(j)}$, $j = 1, \cdots, m$ be U-statistics having expectation $\theta_j$ and kernels $\phi^{(j)}$ of degrees $k_j$. Also let $\Sigma = (\sigma_{i,j})$ where 
    \begin{align*}
        \sigma_{i,j} = k_i k_j Cov(\phi^{(i)} (X_1, \cdots, X_{k_i}), \phi^{(j)} (X_{k_i}, \cdots, X_{k_i + k_j -1})),
    \end{align*}
    and denote by $U_n$ and $\theta$ the m-vectors $(U_n^{(1)}, \cdots, U_n^{(m)})$ and $(\theta_1, \cdots, \theta_m)$ respectively. Then $n^{\frac{1}{2}}(U_n - \theta)$ converges in distribution to a multivariate normal distribution with mean vector zero and covariance matrix $\Sigma$.
    \end{theorem}
    
    We also care about the asymptotic behavior of independent, non-identically distributed random variables, and Hoeffding provided a proof of such CLT in 1948. 
    \begin{theorem}
    Let $X_1, X_2, \cdots,$ be independent r.v.s with distribution functions $F_1, F_2, \cdots$. Define for $i = 1,2, \cdots, n$
    \begin{align}
        \phi_{1,i} (x) = {n-1 \choose k-1}^{-1} \sum_{(n-1, k-1)}^{(-i)} \phi_{1, i; i_2, \cdots, i_k}(x)
    \end{align}
    where the sum is taken over all $(k-1)$-subsets $\{i_2, \cdots, i_k\}$ of $\{1,2, \cdots, n\}$ that do not contain $i$, and 
    \begin{align}
        \phi_{1, i; i_2, \cdots, i_k}(x) = E \phi(x, X_{i2}, \cdots, X_{i_k}) - \theta(i, i_2, \cdots, i_k).
    \end{align}
    Suppose that 
    \begin{enumerate}[label = (\roman*)]
        \item $Var \phi(X_{i_1}, \cdots, X_{i_k} ) < A$ for some constant $A$ and all $i_1, \cdots, i_k$;
        \item For some $\delta > 0, E|\phi_{1, i} (X_i) |^{2 + \delta} < \infty$ for all $i$, and 
        \begin{align}
            \lim_{n \rightarrow \infty} \sum_{i =1}^{n} E|\phi_{1, i} (X_i) |^{2 + \delta} \Big/ (\sum_{i =1}^{n} E|\phi_{1, i} (X_i)^2 |) ^{(2 + \delta)/2} = 0
        \end{align}
       Then $(U_n - E(U_n)) / (Var U_n)^{\frac{1}{2}} \xrightarrow{\mathcal{D}} N(0,1).$ 
    \end{enumerate}
    \end{theorem}
\end{document}