%\documentclass[twocolumn]{article}
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx,algpseudocode}
\usepackage{geometry, amssymb, amsmath, amsthm}
\usepackage{bbm}
\usepackage{enumitem}   
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{chngpage}
\usepackage{breqn}
\usepackage{color,soul}
\usepackage{algorithm}
\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{ref.bib}
\linespread{1.3}
\geometry{margin=1in, headsep=0.25in}

\parindent 0in
\parskip 0.1in

\usepackage[final]{changes}
\setdeletedmarkup{{\color{red}\sout{#1}}}
\usepackage{todonotes}
\newcommand{\kk}[2][]{\todo[color=purple!20,#1]{KK: #2}}

%\usepackage[backend=biber,style=authoryear]{biblatex}
%\bibliography{ref.bib}

% \usepackage[margin=1.0in]{geometry}
% \parskip = 0.1in


%%%%%%%%%%new commands%%%%%%%%%%%%
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\nsum}{{\sum_{i=1}^n}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\V}{{\text{Var}}}
\newcommand{\prob}{{\mathbb{P}}}
\newcommand{\bvar}[1]{\mathbf{#1}} 

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}

\newtheorem{Lemma}{Lemma}

\newtheorem{Def}{Definition}
\numberwithin{Def}{section}
\newtheorem{ex}{Example}
\newtheorem{Prop}{Proposition}
\newtheorem{Cor}[theorem]{Corollary}
\newtheorem{Rmk}{Remark}

%%%%%%%%%%%%%%%%%%%%%%

\title{U-Statistics}
\author{Lily Chou, Kelly Kung, and Zihuan Qiao}
\date{\today}

\begin{document}
\thispagestyle{empty}

\begin{center}
{\LARGE \bf U-Statistics}\\
{\large Lily Chou, Kelly Kung, Zihuan Qiao}\\
May 2, 2019
\end{center}

Throughout this semester in Probability Theory II, we have learned many concepts such as Law of Large Numbers, Central Limit Theorem, and Martingales. U-Statistics is an example from Statistics where many of the Probability concepts can be applied to. We begin the discussion of U-Statistics with some background information.

\section{Introduction}
Before introducing the concept of U-Statistics, we will first define terms and set notation that will be used throughout this handout. 
\begin{Def}{}
We denote $\theta$ as a \textit{functional} defined on a set $\mathcal{F}$ of distribution functions on $\R$, i.e. $\theta = \theta(F), F \in \mathcal{F}$.
\end{Def}

\begin{Def}{}
For every sufficiently large $n$, we say that $\theta$ \text{admits an unbiased estimator} if there is a function $f_n(X_1, \dotsc, X_n)$ on $n$ variables such that \begin{align}\label{unbiased} E(f_n(X_1, \dotsc, X_n)) = \theta(F).\end{align}
\end{Def}

In 1946, Paul Halmos asked two questions that prompted the study of U-Statistics:
\begin{enumerate}
    \item Does there exist an estimator of $\theta$ that will be unbiased whatever the distribution function $F$ may be? Can we characterize the sets $\mathcal{F}$ and the functionals $\theta$ for which the answer is yes?
    \item If such an estimator exists, what is it? If several exist, which is the best?
\end{enumerate}

These questions will be answered throughout this handout. In fact, Halmos answers the part of the first question with the following theorem. 
\begin{theorem}
With a sample of random variables $X_1, \dotsc, X_n$, a functional $\theta$ defined on a set $\mathcal{F}$ of distribution functions admits an unbiased estimator if and only if there is a function $\psi$ of k variables such that 
\begin{align}\label{unbiased_est}
    \theta(F) = \int_{-\infty}^{\infty} \dotsc \int_{-\infty}^{\infty} \psi(x_1, x_2, \dotsc, x_k)dF(x_1) \dotsc dF(x_k)
\end{align}
\end{theorem}

\begin{proof}
Note that for $n \geq k$, if we let $f_n(X_1, \dotsc, X_n) = \psi(X_1, \dotsc, X_k)$, we have that Equation $(2)$ satisfies the condition of unbiasedness in Equation $(1)$. Furthermore, letting $n=k$ implies that Equation $(2)$ is equivalent to Equation $(3)$. Thus $(2) \iff (3)$. 
\end{proof}

\begin{Def}
A functional $\theta(F)$ that satisfies Equation (2) for some function $\psi$ is called a \textit{regular statistical functional of degree k}. Also, the function $\psi$ is called the \textit{kernel} of the functional. 
\end{Def}

In Theorem 1, the kernel function is only evaluated at $k$ random variables, but since we assume that we have independent identically distributed random variables $X_1, \dotsc, X_n$, it intuitively makes sense that we want a symmetric kernel function that is evaluated at all $n$ observations. 

\begin{Def}
A function is \textit{symmetric} if it is invariant under permutations of its arguments. 
\end{Def}

With a symmetric kernel function and a large enough Borel set $E$, we can state the following theorems. First, we state a lemma.

\begin{Lemma}\label{lemA}
Let $\mathcal{F}$ contain all distributions with finite support in E, and let $f$ be a symmetric function of n variables with 
    \begin{align*}
        \int \dotsc \int_{\R_n} f(x_1, \dotsc, x_n) \prod_{i=1}^n dF(x_i) = 0 \text{ for all } F \in \mathcal{F}
    \end{align*}
    Then $f(x_1, \dotsc, x_n) = 0$ whenever $x_i \in E, i = 1, \dotsc, n$. 
\end{Lemma}

\begin{theorem}\label{unique}
Let $\mathcal{F}$ contain all distributions with finite support in $E$, and let $\theta$ be a regular functional satisfying Equation \ref{unbiased_est}. Then up to equality on $E$, there is a unique symmetric unbiased estimator of $\theta$. Here, estimators are identical if they agree on the Borel set $E$.
\end{theorem}
\begin{proof}
Note that the kernel function $\psi(x_1, \dotsc, x_k)$ may not be symmetric, and so we define a symmetric function $\psi^{[n]}(x_1, \dotsc, x_n) = \frac{(n-1)!}{n!}\sum \psi(x_{i_1}, \dotsc, x_{i_k})$ where the sum is taken over all permutations of $\{i_1, \dotsc, i_k\}$ distinct integers from $\{1, \dotsc, n\}$. Since the random variables are i.i.d and $\psi(x_{i_1}, \dotsc, x_{i_k})$ is a kernel function, we know that $\psi^{[n]}$ is unbiased. Consider another symmetric unbiased estimator $f$, and we consider $f - \psi^{[n]}$. By Lemma \ref{lemA}, since both $f$ and $\psi^{[n]}$ are unbiased, we know that $E(f - \psi^{[n]}) = 0$, which implies $f - \psi^{[n]} = 0$, i.e. $f = \psi^{[n]}$. 
\end{proof}

 \begin{theorem}
    Suppose that $E = \R$. Let $\theta$ be a regular functional of degree k defined by Equation \ref{unbiased_est} on a set of $\mathcal{F}$ of distribution functions containing all distributions having finite support. Let $f$ be an unbiased estimate of $\theta$ based on a sample of size n, so that $f$ satisfies Equation \ref{unbiased}. Then $var(f) \geq var(\psi^{[n]})$ for all $F \in \mathcal{F}$. 
    \end{theorem}
    
   \begin{proof}
    Define $f^{[n]}(x_1, \dotsc, x_n) = \frac{1}{n!}\sum_{(n)}f(x_{i_1}, \dotsc, x_{i_n})$ where the sum $\sum_{(n)}$ is taken over all permutations $(i_1, \dotsc, i_n)$ of $\{1, \dotsc, n\}$. Then $f^{[n]}$ is a symmetric unbiased estimator, and so by Theorem \ref{unique} agrees with $\psi^{[n]}$ on $\R$. Since both estimators are unbiased, we just have to show the inequality using second moments. Using the Cauchy-Schwartz Inequality gives $(\psi^{[n]})^2 \leq \frac{1}{n!}\sum_{(n)}f^2(x_{i_1}, \dotsc, x_{i_n})$. Then, taking the expected value of the inequality gives $E((\psi^{[n]})^2) \leq E(f^2(x_{i_1}, \dotsc, x_{i_n}))$. This implies that $var(f) \geq var(\psi^{[n]})$.
    \end{proof}
    
    \section{U-Statistics}
    U-statistics are named due to their unbiasedness property, and the name was coined by Hoeffding in 1948. U-statistics are of the form 
    \begin{align}\label{u_stat}
        U_n = {n \choose k}^{-1}\sum_{(n,k)}\psi(X_{i_1}, \dotsc, X_{i_k})
    \end{align}

Some examples of U-statistics include:
\begin{ex}\textbf{Sample mean}
Let $\mathcal{F}$ be the set of all distributions whose means exist, such that $\mathcal{F}$ contains all distributions that have finite support on $\R$. The mean functional is given by $\theta(F) = \int_{-\infty}^{\infty} x dF(x)$. Using Equation \ref{u_stat}, we have that $k=1$ and $\psi(X_{i_1}) = x_{i_1}$, which gives $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$.
\end{ex}

\begin{ex}\textbf{Sample variance}
Let $\mathcal{F}$ be the set of all distributions with finite second moment, i.e. $\mathcal{F} = \{F: \int |x|^2 dF(x)<\infty\}$. The variance functional is given by $\theta(F) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2}(x_1 - x_2)^2 dF(x_1)dF(x_2)$. Here, we have that $k=2$ and $\psi(X_{i_1}, X_{i_2}) = \frac{1}{2}(x_{i_1} - x_{i_2})^2$, which gives $s_n^2 = {n \choose 2}^{-1}\sum_{1 \leq i < j \leq n}\frac{1}{2}(X_i - X_j)^2$.
\end{ex}

\begin{ex}\textbf{Kendall's Tau} Here, we consider two points $P_1$ and $P_2$ on a plane. We say that $P_1$ and $P_2$ are concordant if the line joining the points have positive slope, and otherwise, they are discordant. Here, we let $\mathcal{F}$ be the set of distribution functions with all absolutely continuous bivariate random vectors. The functional $\tau = \mathbb{P}(P_1, P_2 \text{ concordant}) - \mathbb{P}(P_1, P_2 \text{ discordant})$. We define the kernel $t$ as 
\begin{align*}
    t(P_1, P_2) = \begin{cases} 1 & \text{if $P_1$ and $P_2$ are concordant} \\ -1 & \text{if $P_1$ and $P_2$ are discordant} = sgn(X_1 - X_2)(Y_1 - Y_2).
    \end{cases}
\end{align*}
Note that $E(t(P_1, P_2)) = \mathbb{P}(P_1, P_2 \text{ concordant}) - \mathbb{P}(P_1, P_2 \text{ discordant})$, which verifies that $t(P_1, P_2)$ is indeed a kernel. Also, here, we have $k=2$. Using Equation \ref{u_stat}, we have that the estimator of Kendall's tau is $\tau_n = {n \choose 2}\sum_{(n,2)}t(P_i, P_j)$.
\end{ex}

\subsection{Variance of U-Statistics}
In the case of i.i.d random variables, we can express the variance of U-statistics in terms of conditional expectations. 

\begin{Def}
Given $c = 1, \dotsc, k$, we denote \begin{align}
    \psi_c(x_1, \dotsc, x_c) = E(\psi(x_1, \dotsc, x_c, X_{c+1}, \dotsc X_k))
\end{align}
as the \textit{conditional expectation}. We also denote the variance of the conditional expectation as 
\begin{align}
    \sigma_c^2 = var(\psi_c(X_1, \dotsc, X_c))
\end{align}
where $\sigma_0^2 = 0$. 
\end{Def}
The conditional expectation has a useful theorem. 

\begin{theorem}\label{cond_exp}
The functions $\psi_c$ defined above have the properties
        \begin{enumerate}[label = (\roman*)]
            \item $\psi_c(x_1, \dotsc, x_c) = E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)), \text{ for } 1 \leq c <d \leq k$
            \item $E(\psi_c(X_1, \dotsc, X_c)) = E(\psi(X_1, \dotsc, X_k))$
        \end{enumerate}
\end{theorem}
\begin{proof}
To prove Theorem 4, we rely on the Tower Property. Part (i) is given by \begin{align*}
 E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)) = E(E(\psi(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d|X_1, \dotsc, X_d))|x_1, \dotsc, x_c)  = E(\psi(x_1, \dotsc, x_d)|x_1, \dotsc, x_c)  
\end{align*}
\end{proof}

\section{Martingales for U-statistics}
\subsection{Reverse martingales}
First, recall the definition of a martingale from class:
\begin{Def}
An integrable sequence $\{X_n: n\ge 1\}$ adapted to a filtration $\{F_n: n\ge 0\}$ is a martingale if
\begin{align*}
 E(X_{n+1}|\mathcal{F}_n)=X_n \hspace{0.2cm} a.s., \forall n\ge0.
\end{align*}
\end{Def}

Accordingly, the definition for filtration is:
\begin{Def}
Let $(\Omega, \mathcal{F}, \mathcal{P})$ be a probability space. Let $\{\mathcal{F}_n: n\ge1\}$ be a sequence of sub $\sigma$-algebras such that the sequence is increasing: $\mathcal{F}_1 \sqsubset \mathcal{F}_2 \sqsubset \dotsc \sqsubset \mathcal{F}$. Such a sequence $\{\mathcal{F}_n: n\ge1\}$ is called a filtration. 
\end{Def}
%For a martingale $\{X_n: n\ge 1\}$, without further specification, we usually assume the natural filtration, which is $\mathcal{F}_n=\sigma \{X_1, X_2, \dotsc, X_n\}$.

Similar to the definition for martingales, we can define reverse martingale as the following:
\begin{Def}
An integrable sequence $\{X_n: n\ge 1\}$ adapted to a decreasing sequence of sub $\sigma$-algebras $\{F_n: n\ge 0\}$ is a reverse martingale if
\begin{align*}
 E(X_m|\mathcal{F}_n)=X_n \hspace{0.2cm} a.s., \forall n\ge m.
\end{align*}
\end{Def}

The reverse martingale has nice convergence properties which is stated in theorem \ref{conv_rm}.
\begin{theorem}\label{conv_rm}
Let $\{X_n\}$ be a reverse martingale. Then $X_n$ converges to $E(X_1|\mathcal{F}_{\infty})$ almost surely and in $L_1$. 
\end{theorem}
To prove this theorem, we can apply similar techniques as we used for proving the convergence of martingales and we are going to omit the proof here. The convergence property of the reverse martingale will be quite useful in the proof for strong law of large numbers for the U-Statistics in the next section.

\subsection{U-statistics as martingales}
\begin{theorem}
Let $U_n$ be a sequence of U-statistics based on a kernel $\psi$ satisfying $E|\psi(X_1, \dotsc, X_k)|<\infty$. Then $\{U_n\}_{n=k}^\infty$ is a reverse martingale adapted to the $\sigma$-fields $\mathcal{F}_n=\sigma\{U_n, U_{n+1,\dotsc}\}$.
\end{theorem}

\begin{proof}
The $\sigma$-fields $\mathcal{F}_n$ are decreasing clearly. Because of the inherent symmetry of the kernel $\psi$ and that samples $X_1, \dotsc, X_k$ are i.i.d, we can write:
\begin{align*}
    E(\psi (X_{i_1}, \dotsc, X_{i_k})|\mathcal{F}_n)=E(\psi (X_1, \dotsc, X_k)|\mathcal{F}_n)
\end{align*}
for every subset $\{i_1, \dotsc, i_k\}$ of $\{1,2,\dotsc,n\}$ and hence 
\begin{align*}
    U_n=E(U_n|\mathcal{F}_n)&={n\choose k}^{-1}\sum_{(n,k)}E(\psi(X_{i_1}, \dotsc,X_{i_k})|\mathcal{F}_n)
    =E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_n).
\end{align*}
Then we check the integrability of $U_n$
\begin{align*}
E(|U_n|)&=E(|E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_n)|)\le E(E(|\psi(X_1,\dotsc,X_k)||\mathcal{F}_n))
=E(|\psi(X_1,\dotsc, X_k)|)<\infty
\end{align*}

Let $n\ge m$. Then since $\mathcal{F}_n \subseteq \mathcal{F}_m$,
\begin{align*}
    E(U_m|\mathcal{F}_n)&=E\{E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_m)|\mathcal{F}_n\}
    =E\{\psi(X_1,\dotsc,X_k)|\mathcal{F}_n\}
    =U_n.
\end{align*}
\end{proof}

\begin{theorem}\label{mat}
Let $U_n$ be a sequence of U-statistics based on a kernel $\psi$ satisfying $E|\psi(X_1, \dotsc, X_k)|<\infty$, and let $\mathcal{F}_n=\sigma\{X_1, \dotsc, X_n\}$. Then $\{{n \choose c} H_n^{(c)}\}_{n=c}^\infty$ is a martingale adapted to the $\mathcal{F}_n$ for $c=1,\dotsc, k$.
\end{theorem}
\begin{proof}
The $\sigma$-fields $\mathcal{F}_n$ are increasing clearly. It's also easy to check the integrability of random variables $H_n^{(c)}$: Since $h^{(1)}(x_1)=\psi_1(x_1)-\theta$ and $\psi(X_1)$ is integrable, then $h^{(1)}(X_1)$ is integerable. By
\begin{align*}
h^{(c)}(x_1,\dotsc,x_c)=\psi(x_1,\dotsc,x_c)-\sum_{j+1}^{c-1}\sum_{(c,j)}h^{(j)}(x_{i_1},\dotsc,x_{i_j})-\theta
\end{align*}
Recursively we can get $h^{(j)}(x_{i_1},\dotsc,x_{i_j})$ is integrable and $H_n^{(j)}={n\choose j}h^{(j)}(x_{i_1},\dotsc,x_{i_j})$ is also integrable.
Then we have 
\begin{align*}
E\{{{n+1}\choose c} H_{n+1}^{(c)}|\mathcal{F}_n\}=E\{{{n+1}\choose c}{{n+1}\choose c}^{-1}\sum_{(n+1,c)}h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}=
\sum_{(n+1,c)}E\{h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}
\end{align*}

and $E\{h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}=h^{(c)}(X_{i_1},\dotsc,X_{i_c})$ provided all the indices $i_j\le n$ for all $j \in \{i,\dotsc, c\}$. If there exist $l$ indices with $i_j\ge n+1$, Since the inherent symmetry property of the kernal function, without loss of generality we can assume that the first $l$ random variabels are not measurable, then
\begin{align*}
E\{h^{(c)}(X_{i_1},\dotsc,X_{i_l},X_{i_{l+1}},\dotsc, X_{i_c})|\mathcal{F}_n=E\{h^{(c)}(X_{i_1},\dotsc,X_{i_l},x_{i_{l+1}},\dotsc, x_{i_c})\}=h^{(c)}_{c-l}(x_{i_1},\dotsc,x_{i_c})=0
\end{align*}
The last equality holds because of Theorem 2 (i) for the H-decomposition. Hence
\begin{align*}
E\{{{n+1}\choose c} H_{n+1}^{(c)}|\mathcal{F}_n\}=\sum_{(n,c)}h^{(c)}(X_{i_1},\dotsc,X_{i_c})={n\choose c}H_n^{(c)}
\end{align*}
\end{proof}

\section{SLLN for U-statistics}
\begin{theorem}
Suppose $E|\psi(X_1, \dotsc,X_k)|<\infty$. Then $U_n$ converges a.s. to $\theta$.
\end{theorem}
\begin{proof}
Since $U_n$ is a reverse martingale adapted to the 
\end{proof}

\end{document}

