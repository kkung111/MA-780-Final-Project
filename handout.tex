%\documentclass[twocolumn]{article}
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx,algpseudocode}
\usepackage{geometry, amssymb, amsmath, amsthm}
\usepackage{bbm}
\usepackage{enumitem}   
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{chngpage}
\usepackage{breqn}
\usepackage{color,soul}
\usepackage{algorithm}
\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{ref.bib}
\linespread{1.3}
\geometry{margin=1in, headsep=0.25in}

\parindent 0in
\parskip 0.1in

\usepackage[final]{changes}
\setdeletedmarkup{{\color{red}\sout{#1}}}
\usepackage{todonotes}
\newcommand{\kk}[2][]{\todo[color=purple!20,#1]{KK: #2}}

%\usepackage[backend=biber,style=authoryear]{biblatex}
%\bibliography{ref.bib}

% \usepackage[margin=1.0in]{geometry}
% \parskip = 0.1in


%%%%%%%%%%new commands%%%%%%%%%%%%
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\nsum}{{\sum_{i=1}^n}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\V}{{\text{Var}}}
\newcommand{\prob}{{\mathbb{P}}}
\newcommand{\bvar}[1]{\mathbf{#1}} 

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}

\newtheorem{Lemma}{Lemma}

\newtheorem{Def}{Definition}
\numberwithin{Def}{section}
\newtheorem{ex}{Example}
\newtheorem{Prop}{Proposition}
\newtheorem{Cor}[theorem]{Corollary}
\newtheorem{Rmk}{Remark}

%%%%%%%%%%%%%%%%%%%%%%

\title{U-Statistics}
\author{Lily Chou, Kelly Kung, and Zihuan Qiao}
\date{\today}

\begin{document}
\thispagestyle{empty}

\begin{center}
{\LARGE \bf U-Statistics}\\
{\large Lily Chou, Kelly Kung, Zihuan Qiao}\\
May 2, 2019
\end{center}

Throughout this semester in Probability Theory II, we have learned many concepts such as Law of Large Numbers, Central Limit Theorem, and Martingales. U-Statistics is an example from Statistics where many of the Probability concepts can be applied to. We begin the discussion of U-Statistics with some background information.

\section{Introduction}
Before introducing the concept of U-Statistics, we will first define terms and introduce notation that will be used throughout this handout. 
\begin{Def}{}
We denote $\theta$ as a \textit{functional} defined on a set $\mathcal{F}$ of distribution functions on $\R$, i.e. $\theta = \theta(F), F \in \mathcal{F}$.
\end{Def}

\begin{Def}{}
For every sufficiently large $n$, we say that $\theta$ \text{admits an unbiased estimator} if there is a function $f_n(X_1, \dotsc, X_n)$ on $n$ variables such that \begin{align}\label{unbiased} E(f_n(X_1, \dotsc, X_n)) = \theta(F).\end{align}
\end{Def}

In 1946, Paul Halmos (\cite{halmos1946theory,}) asked two questions that prompted the study of U-Statistics:
\begin{enumerate}
    \item Does there exist an estimator of $\theta$ that will be unbiased whatever the distribution function $F$ may be? Can we characterize the sets $\mathcal{F}$ and the functionals $\theta$ for which the answer is yes?
    \item If such an estimator exists, what is it? If several exist, which is the best?
\end{enumerate}

These questions will be answered throughout this handout. In fact, Halmos answers the part of the first question with the following theorem. 
\begin{theorem}
With a sample of random variables $X_1, \dotsc, X_n$, a functional $\theta$ defined on a set $\mathcal{F}$ of distribution functions admits an unbiased estimator if and only if there is a function $\psi$ of k variables such that 
\begin{align}\label{unbiased_est}
    \theta(F) = \int_{-\infty}^{\infty} \dotsc \int_{-\infty}^{\infty} \psi(x_1, x_2, \dotsc, x_k)dF(x_1) \dotsc dF(x_k)
\end{align}
\end{theorem}

\begin{proof}
Note that for $n \geq k$, if we let $f_n(X_1, \dotsc, X_n) = \psi(X_1, \dotsc, X_k)$, we have that Equation $(2)$ satisfies the condition of unbiasedness in Equation $(1)$. Furthermore, letting $n=k$ implies that Equation $(2)$ is equivalent to Equation $(3)$. Thus $(2) \iff (3)$. 
\end{proof}

\begin{Def}
A functional $\theta(F)$ that satisfies Equation (2) for some function $\psi$ with $k$ variables is called a \textit{regular statistical functional of degree k}. Also, the function $\psi$ is called the \textit{kernel} of the functional. 
\end{Def}

In Theorem 1, the kernel function is only evaluated at $k$ random variables, but since we assume that we have independent identically distributed random variables $X_1, \dotsc, X_n$, it intuitively makes sense that we want a symmetric kernel function that is evaluated at all $n$ observations. 

\begin{Def}
A function is \textit{symmetric} if it is invariant under permutations of its arguments. 
\end{Def}

With a symmetric kernel function and a large enough Borel set $E$, we can state the following theorems. First, we state a lemma.

\begin{Lemma}\label{lemA}
Let $\mathcal{F}$ contain all distributions with finite support in E, and let $f$ be a symmetric function of n variables with 
    \begin{align*}
        \int \dotsc \int_{\R_n} f(x_1, \dotsc, x_n) \prod_{i=1}^n dF(x_i) = 0 \text{ for all } F \in \mathcal{F}
    \end{align*}
    Then $f(x_1, \dotsc, x_n) = 0$ whenever $x_i \in E, i = 1, \dotsc, n$. 
\end{Lemma}

\begin{theorem}\label{unique}
Let $\mathcal{F}$ contain all distributions with finite support in $E$, and let $\theta$ be a regular functional satisfying Equation \ref{unbiased_est}. Then up to equality on $E$, there is a unique symmetric unbiased estimator of $\theta$. Here, estimators are identical if they agree on the Borel set $E$.
\end{theorem}
\begin{proof}
Note that the kernel function $\psi(x_1, \dotsc, x_k)$ may not be symmetric, and so we define a symmetric function $\psi^{[n]}(x_1, \dotsc, x_n) = \frac{(n-1)!}{n!}\sum \psi(x_{i_1}, \dotsc, x_{i_k})$ where the sum is taken over all permutations of $\{i_1, \dotsc, i_k\}$ distinct integers from $\{1, \dotsc, n\}$. Since the random variables are i.i.d and $\psi(x_{i_1}, \dotsc, x_{i_k})$ is a kernel function, we know that $\psi^{[n]}$ is unbiased. Consider another symmetric unbiased estimator $f$, and we consider $f - \psi^{[n]}$. By Lemma \ref{lemA}, since both $f$ and $\psi^{[n]}$ are unbiased, we know that $E(f - \psi^{[n]}) = 0$, which implies $f - \psi^{[n]} = 0$, i.e. $f = \psi^{[n]}$. 
\end{proof}

 \begin{theorem}
    Suppose that $E = \R$. Let $\theta$ be a regular functional of degree k defined by Equation \ref{unbiased_est} on a set of $\mathcal{F}$ of distribution functions containing all distributions having finite support. Let $f$ be an unbiased estimate of $\theta$ based on a sample of size n, so that $f$ satisfies Equation \ref{unbiased}. Then $var(f) \geq var(\psi^{[n]})$ for all $F \in \mathcal{F}$. 
    \end{theorem}
    
   \begin{proof}
    Define $f^{[n]}(x_1, \dotsc, x_n) = \frac{1}{n!}\sum_{(n)}f(x_{i_1}, \dotsc, x_{i_n})$ where the sum $\sum_{(n)}$ is taken over all permutations $(i_1, \dotsc, i_n)$ of $\{1, \dotsc, n\}$. Then $f^{[n]}$ is a symmetric unbiased estimator, and so by Theorem \ref{unique} agrees with $\psi^{[n]}$ on $\R$. Since both estimators are unbiased, we just have to show the inequality using second moments. Using the Cauchy-Schwartz Inequality gives $(\psi^{[n]})^2 \leq \frac{1}{n!}\sum_{(n)}f^2(x_{i_1}, \dotsc, x_{i_n})$. Then, taking the expected value of the inequality gives $E((\psi^{[n]})^2) \leq E(f^2(x_{i_1}, \dotsc, x_{i_n}))$. This implies that $var(f) \geq var(\psi^{[n]})$.
    \end{proof}
 
 \begin{Rmk}
 There is an equivalent theorem with results similar to Theorem 2 and Theorem 3 for when $\mathcal{F}$ contains distribution functions containing all absolutely continuous distribution functions. 
 \end{Rmk}
 
    \section{U-statistics}
    U-statistics are named due to their unbiasedness property, and the name was coined by Hoeffding in 1948 (\cite{hoeffding1948}). U-statistics are of the form 
    \begin{align}\label{u_stat}
        U_n = {n \choose k}^{-1}\sum_{(n,k)}\psi(X_{i_1}, \dotsc, X_{i_k})
    \end{align}

Some examples of U-statistics include:
\begin{ex}\textbf{Sample mean}
Let $\mathcal{F}$ be the set of all distributions whose means exist, such that $\mathcal{F}$ contains all distributions that have finite support on $\R$. The mean functional is given by $\theta(F) = \int_{-\infty}^{\infty} x dF(x)$. Using Equation \ref{u_stat}, we have that $k=1$ and $\psi(X_{i_1}) = x_{i_1}$, which gives $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$.
\end{ex}

\begin{ex}\textbf{Sample variance}
Let $\mathcal{F}$ be the set of all distributions with finite second moment, i.e. $\mathcal{F} = \{F: \int |x|^2 dF(x)<\infty\}$. The variance functional is given by $\theta(F) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2}(x_1 - x_2)^2 dF(x_1)dF(x_2)$. Here, we have that $k=2$ and $\psi(X_{i_1}, X_{i_2}) = \frac{1}{2}(x_{i_1} - x_{i_2})^2$, which gives $s_n^2 = {n \choose 2}^{-1}\sum_{1 \leq i < j \leq n}\frac{1}{2}(X_i - X_j)^2$.
\end{ex}

\begin{ex}\textbf{Kendall's Tau} Here, we consider two points $P_1$ and $P_2$ on a plane. We say that $P_1$ and $P_2$ are concordant if the line joining the points have positive slope, and otherwise, they are discordant. Here, we let $\mathcal{F}$ be the set of distribution functions with all absolutely continuous bivariate random vectors. The functional $\tau = \mathbb{P}(P_1, P_2 \text{ concordant}) - \mathbb{P}(P_1, P_2 \text{ discordant})$. We define the kernel $t$ as 
\begin{align*}
    t(P_1, P_2) = \begin{cases} 1 & \text{if $P_1$ and $P_2$ are concordant} \\ -1 & \text{if $P_1$ and $P_2$ are discordant} = sgn(X_1 - X_2)(Y_1 - Y_2).
    \end{cases}
\end{align*}
Note that $E(t(P_1, P_2)) = \mathbb{P}(P_1, P_2 \text{ concordant}) - \mathbb{P}(P_1, P_2 \text{ discordant})$, which verifies that $t(P_1, P_2)$ is indeed a kernel. Also, here, we have $k=2$. Using Equation \ref{u_stat}, we have that the estimator of Kendall's tau is $\tau_n = {n \choose 2}\sum_{(n,2)}t(P_i, P_j)$.
\end{ex}

\subsection{Variance of U-Statistics}
In the case of i.i.d random variables, we can express the variance of U-statistics in terms of conditional expectations. 

\begin{Def}
Given $c = 1, \dotsc, k$, we denote \begin{align}
    \psi_c(x_1, \dotsc, x_c) = E(\psi(x_1, \dotsc, x_c, X_{c+1}, \dotsc X_k)) = E(\psi(X_1, \dotsc, X_k)|X_1 = x_1, \dotsc, X_c = x_c)
\end{align}
as the \textit{conditional expectation}. We also denote the variance of the conditional expectation as 
\begin{align}
    \sigma_c^2 = var(\psi_c(X_1, \dotsc, X_c))
\end{align}
where $\sigma_0^2 = 0$. 
\end{Def}

\begin{theorem}\label{cond_exp}
The functions $\psi_c$ defined above have the properties
        \begin{enumerate}[label = (\roman*)]
            \item $\psi_c(x_1, \dotsc, x_c) = E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)), \text{ for } 1 \leq c <d \leq k$
            \item $E(\psi_c(X_1, \dotsc, X_c)) = E(\psi(X_1, \dotsc, X_k))$
        \end{enumerate}
\end{theorem}
\begin{proof}
To prove Theorem \ref{cond_exp}, we rely on the Tower Property. Part (i) is given by 
\begin{align*}
 E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)) &= E(E(\psi(X_1, \dotsc, X_k)|x_1, \dotsc, x_d)|x_1, \dotsc, x_c)  \\
 &= E(\psi(X_1, \dotsc, X_k)|x_1, \dotsc, x_c)  &&\text{[since $c<d$]}\\
 &=\psi_c(x_1, \dotsc, x_c)
\end{align*}

Part (ii) is also given be the Tower Property, where we have 
\begin{align*}
    E(\psi_c(x_1, \dotsc, x_c)) &= E(E(\psi(X_1, \dotsc, X_k|x_1, \dotsc, x_c)) = E(\psi(X_1, \dotsc, X_k)
\end{align*}
\end{proof}

\begin{theorem}\label{cond_exp_prop}
We can express the variance of the conditional expectation as \begin{align}
    \sigma_c^2 = cov(\psi(S_1), \psi(S_2))
\end{align}
where $S_1, S_2$ are two subsets of size $k$ from $\{1, \dotsc, n\}$ with $c$ elements in common. 
\end{theorem}

\begin{proof}
By Theorem \ref{cond_exp} part (ii), we know that $E(\psi_c(X_1, \dotsc, X_c)) = E(\psi(X_1, \dotsc, X_k))$, so we just have to show $E(\psi(X_1, \dotsc, X_k)\psi(X_1, \dotsc, X_c, X_{k+1}, \dotsc, X_{2k-c})) = E\left[(\psi_c(X_1, \dotsc, X_c))^2\right]$. We thus have the following:
\begin{fontsize}{8}{12}
\begin{align*}
    E(\psi(X_1, \dotsc, X_k)\psi(X_1, \dotsc, X_c, X_{k+1}, \dotsc, X_{2k-c}))&= E\left[E(\psi(X_1, \dotsc, X_k)\psi(X_1, \dotsc, X_c, X_{k+1}, \dotsc, X_{2k-c})|X_1, \dotsc, X_c)\right]\\
    &=E\left[E(\psi(X_1, \dotsc, X_k)|X_1, \dotsc, X_c) E(\psi(X_1, \dotsc, X_c, X_{k+1}, \dotsc, X_{2k-c})|X_1, \dotsc, X_c)\right]\\
    &=E\left[\psi_c(X_1, \dotsc, X_c)^2\right]
\end{align*}
\end{fontsize}Thus, we have $cov(\psi(S_1)\psi(S_2)) = E\left[\psi_c(X_1, \dotsc, X_c)^2\right] - E\left[\psi_c(X_1, \dotsc, X_c)\right]^2 = \sigma_c^2$.
\end{proof}

Using the results from Theorem \ref{cond_exp_prop}, we can derive the variance for the U-statistic.

\begin{theorem}\label{var_u_stat}
Let $U_n$ be a U-statistic with kernel $\psi$ of degree k. Then, 
\begin{align}
    var(U_n) = {n \choose k}^{-1}\sum_{c=1}^k{k \choose c}{n-k \choose k-c} \sigma_c^2
\end{align}
\end{theorem}

\begin{proof}
    \begin{align*}
        var(U_n) &= var({n \choose k}^{-1} \sum_{(n,k)}\psi(X_{i_1}, \dotsc, X_{i_n})) = cov({n \choose k}^{-1} \sum_{(n,k)}\psi(X_{i_1}, \dotsc, X_{i_n}), {n \choose k}^{-1} \sum_{(n,k)}\psi(X_{j_1}, \dotsc, X_{j_n}))\\
        &= {n \choose k}^{-2}\sum_{(n,k)}\sum_{(n,k)}cov(\psi(X_{i_1}, \dotsc, X_{i_n}), \psi(X_{j_1}, \dotsc, X_{j_n})) ={n \choose k}^{-1}\sum_{c=1}^k {k \choose c}{n-k \choose k-c}\sigma_c^2
    \end{align*}
by using the results in Theorem \ref{cond_exp_prop}. The coefficients come from counting the number of ways of choosing two subsets of size $k$ with $c$ elements in common. 
\end{proof}

\subsection{Covariance of U-statistics}
Suppose now we have two U-statistics $U_n^{(1)}$ and $U_n^{(2)}$ where they are based on the same sample $X_1, \dotsc, X_n$, but they have different kernel functions, $\psi$ and $\phi$ of degrees $k_1$ and $k_2$, respectively. Assume that WLOG, $k_1 \leq k_2$. 

We first define $\sigma_{c,d}^2 = cov(\psi_c(X_1, \dotsc, X_c), \phi_d(X_1, \dotsc, X_d))$. Using this, we can define the covariance of U-statistics. 
\begin{theorem}
Suppose that $c \leq d$. If $S_1$ is in $S_{n,k_1}$ and $S_2$ in $S_{n, k_2}$ with $|S_1 \cap S_2| = c$, then 
    \begin{align}
    \sigma_{c,d}^2 = cov(\psi(S_1), \phi(S_2))
    \end{align}
\end{theorem}

\begin{proof}
    The proof is very similar to that of the Theorem \ref{cond_exp_prop} and uses the result from Theorem \ref{cond_exp}. However, in the product term, we condition on $d$ terms instead of $c$ terms. 
\end{proof}

\begin{Rmk}
 As a consequence, we have $\sigma_{c,c}^2 = \sigma_{c,c+1}^2 = \dotsc = \sigma_{c,k_2}^2$ for $c = 1, \dotsc, k_1$.    
\end{Rmk}
    
    \begin{theorem}
    Let $U_n^{(1)}$ and $U_n^{(2)}$ be defined as above. Then, 
    \begin{align}
        cov(U_n^{(1)}, U_n^{(2)}) = {n \choose k_1}^{-1}\sum_{c=1}^{k_1}{k_2 \choose c}{n-k_2 \choose k_1 - c}\sigma_{c,c}^2
        \end{align}
    \end{theorem}
    \begin{proof}
    \begin{align*}
      cov(U_n^{(1)}, U_n^{(2)}) &= {n \choose k_1}^{-1}{n \choose k_2}^{-1}\sum_{(n,k_1)}\sum_{(n, k_2)} cov(\psi(S_1), \phi(S_2)) = {n \choose k_1}^{-1}{n \choose k_2}^{-1}\sum_{c=1}^{k_1}\sum_{|S_1 \cap S_2| = c} cov(\psi(S_1), \phi(S_2)) \\
      &= {n \choose k_1}^{-1}\sum_{c=1}^{k_1}{k_2 \choose c}{n-k_2 \choose k_1 - c}\sigma_{c,c}^2
    \end{align*}
since there are ${n \choose k_2}{k_2 \choose c}{n-k_2 \choose k_1 - c}$ ways to choose two subsets $S_1, S_2$ with $k_1$ and $k_2$ elements from the sample, where $|S_1 \cap S_2| = c$.
    \end{proof}
    
\subsection{H-Decomposition}
U-statistics of degree $k$ can be decomposed in terms of a sum of uncorrelated U-statistics of degree $1, 2, \dotsc, k$ that has variances of decreasing order in $n$ (\cite{hoeffding1961strong}). Using this decomposition, it is easier to prove some of the  results discussed later on in this handout. 

We first introduce kernels $h^{(1)}, \dotsc, h^{(k)}$ of degrees $1, \dotsc, k$ that are defined recursively: 
\begin{align}
    h^{(1)}(x_1) &= \psi_1(x_1) - \theta \notag \\
    h^{(c)}(x_1, \dotsc, x_c) &= \psi_c(x_1, \dotsc, x_c) - \sum_{j=1}^{c-1}\sum_{(c,j)}h^{(j)}(x_{i_1}, \dotsc, x_{i_j}) - \theta \label{h_recursive}
\end{align}
where $c = 2, \dotsc, k$.

\begin{theorem}
For $j = 1, \dotsc, k$, let $H_n^{(j)}$ be the U-statistic based on the kernel $h^{(j)}$, where \begin{align}
H_n^{(j)} = {n \choose j}^{-1} \sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j}). 
\end{align}
Then,
\begin{align}
    U_n  = \theta + \sum_{j=1}^k{k \choose j}H_n^{(j)}
\end{align}
\end{theorem}
\begin{proof}
First, let $S_j\{i_1, \dotsc, i_k\}$ denote the sum $\sum h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j})$, which is summed over all $j$-subsets $\{\nu_1, \dotsc, \nu_j\}$ of $\{1, \dotsc, n\}$. Then, we have the relationship $\sum_{(n,k)}S_j\{i_1, \dotsc, i_k\} = {n-j \choose k-j}\sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j})$, which holds due to the identity ${n-j \choose k-j}{n \choose j} = {n \choose k}{k \choose j} \implies {n \choose k}^{-1}{n-j \choose k-j} = {k \choose j}{n \choose j}^{-1}$. We can then decompose $U_n$ as follows.
\begin{align*}
    U_n &= {n \choose k}^{-1}\sum_{(n,k)}\psi(x_{i_1}, \dotsc, x_{i_k}) = {n \choose k}^{-1}\sum_{(n,k)}\left[\sum_{j=1}^kS_j\{i_1, \dotsc, i_k\} + \theta \right] &&\text{[from Eq. \ref{h_recursive} with $c = k$]} \\
    &= \theta + {n \choose k}^{-1}\sum_{j=1}^k{n-j \choose k-j}\sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j}) = \theta + {n \choose k}^{-1}\sum_{j=1}^k{k \choose j}H_n^{(j)}
\end{align*}
\end{proof}

\begin{Rmk}
We can also truncate the H-decomposition up to $c$ terms, and we are then left with a remainder term $R_n^{(c)}$, i.e. 
\begin{align}
    U_n = \theta + \sum_{j=1}^c{k \choose j} H_n^{(c)} + R_n^{(c)}
\end{align}
where $R_n^{(c)}$ is a U-statistic with kernel $\sum_{j=c+1}^k S_j\{1, \dotsc, k\}$.  
\end{Rmk}

\begin{Rmk}
We can represent the functions $h^{(c)}$ in a slightly different manner. First, let $G_x$ denote the distribution function of a single point mass at $x$. Then, we have 
\begin{align}
    h^{(1)}(x_1) - \psi(x_1) - \theta  &= \int \dotsc \int \psi(x_1, u_2, \dotsc, d_k)\prod_{i=2}^k dF(u_i) - \int \dotsc \int \psi(u_1, u_2, \dotsc, d_k)\prod_{i=1}^k dF(u_i) \notag \\
    &= \int \dotsc \int \psi(u_1, \dotsc, u_k)(dG_{x_1}(u_1) - dF(u_1))\prod_{i=2}^k dF(u_i) \notag \\
    h^{(j)}(x_1, \dotsc, x_j) &= \int \dotsc \int \psi(u_1, \dotsc, u_k) \prod_{i=1}^j(dG_{x_i}(u_i) - dF(u_i))\prod_{j+1}^kdF(u_i) \label{h_func_rep}
\end{align}
Here, the relation in Equation \ref{h_func_rep} is derived by taking the product of $dG_{x_i}(u_i)$. In particular, we have $\prod_{i=1}^kdG_{x_i}(u_i) = \prod_{i=1}^k\left[(dG_{x_i}(u_i) - dF(u_i)) + dF(u_i) \right]$, which leads to $$\prod_{i=1}^kdG_{x_i}(u_i) = \sum_{c=0}^k \sum_{(k,c)}\prod_{j=1}^c\left(dG_{x_{i_j}}(u_{i_j}) - dF(u_{i_j})\right)\prod_{c+1}^k dF(u_{i'_j}).$$
\end{Rmk}

\begin{theorem}
    \begin{enumerate} [label = (\roman*)]
        \item For $c = 1, \dotsc, j-1$ and $j = 1, \dotsc, k$, $h_c^{(j)}(x_1, \dotsc, x_c) = 0$
        \item $E(h^{(j)}(X_1, \dotsc, X_j)) = 0$
    \end{enumerate}
    \end{theorem}
    
    \begin{proof}
    By Definition 2.1, we can write $$h_{j-1}^{(j)} = E\{h^{(j)} (x_1, \cdots, x_{j-1}, X_j)\}$$
    then by equation (14), we can write 
    \begin{align*}
        h^{(j)}(x_1, \dotsc, x_j) = \int \dotsc \int &\psi(u_1, \dotsc, u_k) \prod_{i=1}^j(dG_{x_i}(u_i) - dF(u_i))\prod_{j+1}^kdF(u_i) \\
        = \int \dotsc \int &\psi(u_1, \dotsc, x_j, \cdots, u_k) \prod_{i=1}^{j-1}(dG_{x_i}(u_i) - dF(u_i))\prod_{i =j+1}^k dF(u_i) \\
        - &\int \dotsc \int \psi(u_1, \dotsc, u_k) \prod_{i=1}^{j-1}(dG_{x_i}(u_i) - dF(u_i))\prod_{i =j}^kdF(u_i) \\
        = \int \dotsc \int &\psi(u_1, \cdots, u_k) \prod_{i=1}^{j-1}(dG_{x_i}(u_i) - dF(u_i))\prod_{i =j+1}^k dF(u_i) - h^{(j-1)}(x_1, \dotsc, x_{j-1}).\label{h_func_rep}
    \end{align*}
    Integrating both sides of equation (15) with respect to $\mu_j$ gives us 
    \begin{align*}
        E\{h^{(j)}(x_1, \dotsc, x_{j-1}, X_j) \} &= h^{(j-1)}(x_1, \dotsc, x_{j-1})  - h^{(j-1)}(x_1, \dotsc, x_{j-1})\\
        &= 0.
    \end{align*}
    Then by Theorem 4,
    \begin{align*}
        h_c^{(j)}(x_1, \dotsc, x_c) &= E\{h_{j-1}^{(j)}(x_1, \dotsc, x_c, X_{c+1}, \cdots, X_{j-1}\} \\
        &= 0,
    \end{align*}
    this completes the proof for part (i). 
    Using Theorem 4 part (ii) we see that 
    \begin{align}
        E\{h^{(j)}( X_{1}, \cdots, X_{j}\} &= E\{h_c^{(j)}( X_{1}, \cdots, X_{c}\} = 0.
    \end{align}
    This shows that the variance of the conditional expectation of the $h^{(j)}$ are all zero, except the jth.
    \end{proof}
 We now define 
    \begin{align}
        \delta_j^2 = var(h_j^{(j)}(X_1, \dotsc, X_j)) = var(h^{(j)}(X_1, \dotsc, X_j))
    \end{align}
    
    Then, we have that 
    \begin{align}
        var(H_n^{(j)}) = {n \choose j}^{-1}\sum_{c=1}^j {j \choose c}{n-j \choose j-c}var(h_c^{(j)}(X_1, \dotsc X_c)) = {n \choose j}^{-1}\delta_j^2
    \end{align}
    
    It is important to notice that we use H-decomposition on the U-statistics because the variance of the component U- statistics $H_n^{(j)}$ are of order $n^{-j}$, and they're uncorrelated. To see the uncorrelatedness, the book proposed another theorem as follows:
    
    \begin{theorem}
    \begin{enumerate} [label = (\roman*)]
        \item Let $j <j'$ and let $S_1$ and $S_2$ be a j-subset of $\{1, \dotsc, n\}$ and a j'-subset of $\{1, \dotsc, n\}$, respectively. Then, $cov(h^{(j)}(S_1), h^{(j')}(S_2)) = 0$ and so $cov(H_n^{(j)}, H_n^{(j')}) = 0$
        \item Let $S_1 \neq S_2$ be two distinct j-subsets of $\{1, \dotsc, n\}$. Then, $cov(h^{(j)}(S_1), h^{(j)}(S_2)) = 0$
    \end{enumerate}
    \end{theorem}
    
    
\begin{proof}

\end{proof}

    
    \begin{theorem}
    The variance of $U_n$ is given by 
    \begin{align}
        var(U_n) = \sum_{j=1}^k {k \choose j}^2 {n \choose j}^{-1}\delta_j^2
    \end{align}
    \end{theorem}
    \begin{proof}
    \begin{align*}
        var(U_n) &= var(\theta + \sum_{j=1}^k {k \choose j}H_n^{(j)} \\
        &= \sum_{j=1}^k {k \choose j}^2var(H_n^{(j)})\\
        &= \sum_{j=1}^k {k \choose j}^2 {n \choose j}^{-1}\delta_j^2
    \end{align*}
    \end{proof}

    Relations describing $\sigma_{c}^2$ in terms of the $\delta_j^2 $ and vice versa can be derived as follows: from the definition of H-decomposition, we have 
    \begin{align*}
        \phi_c (X_1, \cdots, X_c) = \sum_{j = 1}^c \sum_{c,j} h^{(j)} (X_{i_{1}}, \cdots , X_{i_j}) + \theta
    \end{align*}
    thus take variance of both sides, we get 
    \begin{align*}
        \sigma_{c}^2 &= \sum_{j = 1}^c Var\{ \sum_{(c, j)} h^{(j)} (X_{i_{1}}, \cdots , X_{i_j})\} \\
        &= \sum_{j = 1}^c {c \choose j}^2 Var( H_c^{(j)}) \\ 
        &= \sum_{j = 1}^c {c \choose j} \delta_j^2.
    \end{align*}
    
    By using binomial coefficients, we could try to invert the last expression. Multiply both sides of the equation above by  $(-1)^{d-c} {d \choose c}$, and taking the sum from $c = 1$ to $d$ gives us
    \begin{align*}
        \sum_{c=1}^d {d \choose c} (-1)^{d-c} \sigma_{c}^2 &= \sum_{c=1}^d  
       \sum_{j = 1}^c {c \choose j}  {d \choose c} (-1)^{d-c} \delta_j^2 \\
       &= \sum_{j=1}^d \left\{ \sum_{c = 0}^{d - j} {d - j \choose c} (-1)^c \right\} (-1)^{d -j} {d \choose j} \delta_j^2.
    \end{align*}
    The term inside the braces is zero except when $j = d$; hence we get 
    \begin{align*}
        \delta_d^2 = \sum_{c = 1}^d (-1)^{b-c} {d \choose c} \sigma_{c}^2
    \end{align*}
    The reason we care about $\delta_d^2$ is because if a U-statistics is degenerate of order $d$, i.e., if $0 = \sigma_1^2 = \cdots \sigma_d^2 < \sigma_{d+1}^2$, then the first $d$ terms of the H-decomposition vanish, since in this case $\delta_1^2 = \cdots = \delta_d^2 = 0.$ When we discuss asymptotic of U-statistics later on, we'd see that the order of degeneracy determines the asymptotic distribution of the U-statistics.  
    
    
    
    \section{Asymptotics} 
    Many of the classical results of probability theory dealing with of sums of independent and identically distributed random variables have generalizations to U-statistics. In this section we present two versions of Central Limit theorem, one is the case of i.i.d. r.v.s, the second is the case where we no longer have identically distributed, but the r.v.s still remain independent. 
    The main tool for this study is H-decomposition. We start with the class of U-statistics with the simplest asymptotic behavior, those that are asymptotically normally distributed. 
    
    \begin{theorem}
    Let $\sigma_1^{2} > 0$. Then $n^{\frac{1}{2}}(U_n - \theta)$ is asymptotically normal with mean zero and asymptotic variance $k^2 \sigma_1^2$.
    \end{theorem}
    
    \begin{proof}
      First, by definition of H-decomposition, we have 
      \begin{align*}
          n^{\frac{1}{2}}(U_n - \theta) &= n^{\frac{1}{2}} \left( {k \choose 1} H_n^{(1)} + R_n \right) \\
          &= n^{\frac{1}{2}} \left( k {n \choose 1}^{-1} \sum_{ (n,1) } h^{(1)} + R_n \right) \\
          &= \frac{\sqrt{n}}{n} k \sum_{i = 1}^{n} (x_i) + \sqrt{n}R_n \\
          &= \frac{k}{\sqrt{n}} \sum_{i = 1}^{n} (x_i) + \sqrt{n}R_n. 
      \end{align*}
      Note that $\sum_{i = 1}^{n} (x_i)$ is i.i.d by assumption, and $$ E (\frac{k}{\sqrt{n}} \sum_{i = 1}^{n} (x_i)) = \frac{k}{\sqrt{n}} E( \sum_{i = 1}^{n} (x_i) )  = 0$$
      by Theorem 10 (ii). Similarly, we have $$E(\sqrt{n}R_n) = \sqrt{n} \sum_{j = 2}^k {k \choose 1} E(H_n^{(j)}) = 0,$$
      thus $E\left( n^{\frac{1}{2}}(U_n - \theta) \right) = 0. $ 
      Next, we look at the variance of $n^{\frac{1}{2}}(U_n - \theta)$. By orthogonality of $H_n$ and $R_n$,
      \begin{align*}
          Var(n^{\frac{1}{2}}(U_n - \theta) ) &= Var(n^{\frac{1}{2}} U_n) + Var(\sqrt{n}R_n) \\
          &= \frac{k^2}{n} \sum_{i = 1}^n Var(h^{(1)} (x_i) )  + n Var(R_n) \\
          &= \frac{k^2}{n} n \sigma_1^2 + n Var( \sum_{j = 2}^k {k \choose j} H_n^{(j)}) \\
          &= k^2 \sigma^2 + n \sum_{j = 2}^k {k \choose j}^2 Var(H_n^{(j)}) \\
          &= k^2 \sigma^2 + n \sum_{j = 2}^k {k \choose j}^2 {n \choose j}^{-1} \delta_j^2 \\
          &= k^2 \sigma^2 + \mathcal{O}(n^{-1}).
      \end{align*}
      Hence asymptotically we have $Var(n^{\frac{1}{2}}(U_n - \theta) ) = k^2 \sigma^2$. 
      
    \end{proof}
    
    
    We also care about the asymptotic behavior of independent, non-identically distributed random variables, and Hoeffding provided a proof of such CLT in 1948. 
    \begin{theorem}
    Let $X_1, X_2, \cdots,$ be independent r.v.s with distribution functions $F_1, F_2, \cdots$. Define for $i = 1,2, \cdots, n$
    \begin{align}
        \phi_{1,i} (x) = {n-1 \choose k-1}^{-1} \sum_{(n-1, k-1)}^{(-i)} \phi_{1, i; i_2, \cdots, i_k}(x)
    \end{align}
    where the sum is taken over all $(k-1)$-subsets $\{i_2, \cdots, i_k\}$ of $\{1,2, \cdots, n\}$ that do not contain $i$, and 
    \begin{align}
        \phi_{1, i; i_2, \cdots, i_k}(x) = E \phi(x, X_{i2}, \cdots, X_{i_k}) - \theta(i, i_2, \cdots, i_k).
    \end{align}
    Suppose that 
    \begin{enumerate}[label = (\roman*)]
        \item $Var \phi(X_{i_1}, \cdots, X_{i_k} ) < A$ for some constant $A$ and all $i_1, \cdots, i_k$;
        \item For some $\delta > 0, E|\phi_{1, i} (X_i) |^{2 + \delta} < \infty$ for all $i$, and 
        \begin{align}
            \lim_{n \rightarrow \infty} \sum_{i =1}^{n} E|\phi_{1, i} (X_i) |^{2 + \delta} \Big/ (\sum_{i =1}^{n} E|\phi_{1, i} (X_i)^2 |) ^{(2 + \delta)/2} = 0
        \end{align}
       Then $(U_n - E(U_n)) / (Var U_n)^{\frac{1}{2}} \xrightarrow{\mathcal{D}} N(0,1).$ 
    \end{enumerate}
    \end{theorem}
\section{Martingales for U-statistics}
\subsection{Reverse martingales}
First, recall the definition of a martingale from class:
\begin{Def}
An integrable sequence $\{X_n: n\ge 1\}$ adapted to a filtration $\{F_n: n\ge 0\}$ is a martingale if
\begin{align*}
 E(X_{n+1}|\mathcal{F}_n)=X_n \hspace{0.2cm} a.s., \forall n\ge0.
\end{align*}
\end{Def}

Accordingly, the definition for filtration is:
\begin{Def}
Let $(\Omega, \mathcal{F}, \mathcal{P})$ be a probability space. Let $\{\mathcal{F}_n: n\ge1\}$ be a sequence of sub $\sigma$-algebras such that the sequence is increasing: $\mathcal{F}_1 \sqsubset \mathcal{F}_2 \sqsubset \dotsc \sqsubset \mathcal{F}$. Such a sequence $\{\mathcal{F}_n: n\ge1\}$ is called a filtration. 
\end{Def}
%For a martingale $\{X_n: n\ge 1\}$, without further specification, we usually assume the natural filtration, which is $\mathcal{F}_n=\sigma \{X_1, X_2, \dotsc, X_n\}$.

Similar to the definition for martingales, we can define reverse martingale as the following:
\begin{Def}
An integrable sequence $\{X_n: n\ge 1\}$ adapted to a decreasing sequence of sub $\sigma$-algebras $\{F_n: n\ge 0\}$ is a reverse martingale if
\begin{align*}
 E(X_m|\mathcal{F}_n)=X_n \hspace{0.2cm} a.s., \forall n\ge m.
\end{align*}
\end{Def}

The reverse martingale has nice convergence properties which is stated in theorem \ref{conv_rm}.
\begin{theorem}\label{conv_rm}
Let $\{X_n\}$ be a reverse martingale. Then $X_n$ converges to $E(X_1|\mathcal{F}_{\infty})$ almost surely and in $L_1$. 
\end{theorem}
To prove this theorem, we can apply similar techniques as we used for proving the convergence of martingales and we are going to omit the proof here. The convergence property of the reverse martingale will be quite useful in the proof for strong law of large numbers for the U-Statistics in the next section.

\subsection{U-statistics as martingales}
\begin{theorem}
Let $U_n$ be a sequence of U-statistics based on a kernel $\psi$ satisfying $E|\psi(X_1, \dotsc, X_k)|<\infty$. Then $\{U_n\}_{n=k}^\infty$ is a reverse martingale adapted to the $\sigma$-fields $\mathcal{F}_n=\sigma\{U_n, U_{n+1,\dotsc}\}$.
\end{theorem}

\begin{proof}
The $\sigma$-fields $\mathcal{F}_n$ are decreasing clearly. Because of the inherent symmetry of the kernel $\psi$ and that samples $X_1, \dotsc, X_k$ are i.i.d, we can write:
\begin{align*}
    E(\psi (X_{i_1}, \dotsc, X_{i_k})|\mathcal{F}_n)=E(\psi (X_1, \dotsc, X_k)|\mathcal{F}_n)
\end{align*}
for every subset $\{i_1, \dotsc, i_k\}$ of $\{1,2,\dotsc,n\}$ and hence 
\begin{align*}
    U_n=E(U_n|\mathcal{F}_n)&={n\choose k}^{-1}\sum_{(n,k)}E(\psi(X_{i_1}, \dotsc,X_{i_k})|\mathcal{F}_n)
    =E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_n).
\end{align*}
Then we check the integrability of $U_n$
\begin{align*}
E(|U_n|)&=E(|E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_n)|)\le E(E(|\psi(X_1,\dotsc,X_k)||\mathcal{F}_n))
=E(|\psi(X_1,\dotsc, X_k)|)<\infty
\end{align*}

Let $n\ge m$. Then since $\mathcal{F}_n \subseteq \mathcal{F}_m$,
\begin{align*}
    E(U_m|\mathcal{F}_n)&=E\{E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_m)|\mathcal{F}_n\}
    =E\{\psi(X_1,\dotsc,X_k)|\mathcal{F}_n\}
    =U_n.
\end{align*}
\end{proof}

\begin{theorem}\label{mat}
Let $U_n$ be a sequence of U-statistics based on a kernel $\psi$ satisfying $E|\psi(X_1, \dotsc, X_k)|<\infty$, and let $\mathcal{F}_n=\sigma\{X_1, \dotsc, X_n\}$. Then $\{{n \choose c} H_n^{(c)}\}_{n=c}^\infty$ is a martingale adapted to the $\mathcal{F}_n$ for $c=1,\dotsc, k$.
\end{theorem}
\begin{proof}
The $\sigma$-fields $\mathcal{F}_n$ are increasing clearly. It's also easy to check the integrability of random variables $H_n^{(c)}$: Since $h^{(1)}(x_1)=\psi_1(x_1)-\theta$ and $\psi(X_1)$ is integrable, then $h^{(1)}(X_1)$ is integerable. By
\begin{align*}
h^{(c)}(x_1,\dotsc,x_c)=\psi_c(x_1,\dotsc,x_c)-\sum_{j+1}^{c-1}\sum_{(c,j)}h^{(j)}(x_{i_1},\dotsc,x_{i_j})-\theta
\end{align*}
Recursively we can get $h^{(j)}(x_{i_1},\dotsc,x_{i_j})$ is integrable and $H_n^{(j)}={n\choose j}h^{(j)}(x_{i_1},\dotsc,x_{i_j})$ is also integrable.
Then we have 
\begin{align*}
E\{{{n+1}\choose c} H_{n+1}^{(c)}|\mathcal{F}_n\}=E\{{{n+1}\choose c}{{n+1}\choose c}^{-1}\sum_{(n+1,c)}h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}=
\sum_{(n+1,c)}E\{h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}
\end{align*}

and $E\{h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}=h^{(c)}(X_{i_1},\dotsc,X_{i_c})$ provided all the indices $i_j\le n$ for all $j \in \{i,\dotsc, c\}$. If there exist $l$ indices with $i_j\ge n+1$, Since the inherent symmetry property of the kernal function, without loss of generality we can assume that the first $l$ random variabels are not measurable, then
\begin{align*}
E\{h^{(c)}(X_{i_1},\dotsc,X_{i_l},X_{i_{l+1}},\dotsc, X_{i_c})|\mathcal{F}_n=E\{h^{(c)}(X_{i_1},\dotsc,X_{i_l},x_{i_{l+1}},\dotsc, x_{i_c})\}=h^{(c)}_{c-l}(x_{i_1},\dotsc,x_{i_c})=0
\end{align*}
The last equality holds because of Theorem 2 (i) for the H-decomposition. Hence
\begin{align*}
E\{{{n+1}\choose c} H_{n+1}^{(c)}|\mathcal{F}_n\}=\sum_{(n,c)}h^{(c)}(X_{i_1},\dotsc,X_{i_c})={n\choose c}H_n^{(c)}
\end{align*}
\end{proof}

\section{SLLN for U-statistics}
\begin{theorem}
Suppose $E|\psi(X_1, \dotsc,X_k)|<\infty$. Then $U_n$ converges a.s. to $\theta$.
\end{theorem}
\begin{proof}
Since $U_n$ is a reverse martingale adapted to the 
\end{proof}

\section{Martingales for U-statistics}
\subsection{Reverse martingales}
First, recall the definition of a martingale from class:
\begin{Def}
An integrable sequence $\{X_n: n\ge 1\}$ adapted to a filtration $\{F_n: n\ge 0\}$ is a martingale if
\begin{align*}
 E(X_{n+1}|\mathcal{F}_n)=X_n \hspace{0.2cm} a.s., \forall n\ge0.
\end{align*}
\end{Def}

Accordingly, the definition for filtration is:
\begin{Def}
Let $(\Omega, \mathcal{F}, \mathcal{P})$ be a probability space. Let $\{\mathcal{F}_n: n\ge1\}$ be a sequence of sub $\sigma$-algebras such that the sequence is increasing: $\mathcal{F}_1 \sqsubset \mathcal{F}_2 \sqsubset \dotsc \sqsubset \mathcal{F}$. Such a sequence $\{\mathcal{F}_n: n\ge1\}$ is called a filtration. 
\end{Def}
%For a martingale $\{X_n: n\ge 1\}$, without further specification, we usually assume the natural filtration, which is $\mathcal{F}_n=\sigma \{X_1, X_2, \dotsc, X_n\}$.

Similar to the definition for martingales, we can define reverse martingale as the following:
\begin{Def}
An integrable sequence $\{X_n: n\ge 1\}$ adapted to a decreasing sequence of sub $\sigma$-algebras $\{F_n: n\ge 0\}$ is a reverse martingale if
\begin{align*}
 E(X_m|\mathcal{F}_n)=X_n \hspace{0.2cm} a.s., \forall n\ge m.
\end{align*}
\end{Def}

The reverse martingale has nice convergence properties which is stated in theorem \ref{conv_rm}.
\begin{theorem}\label{conv_rm}
Let $\{X_n\}$ be a reverse martingale. Then $X_n$ converges to $E(X_1|\mathcal{F}_{\infty})$ almost surely and in $L_1$. 
\end{theorem}
To prove this theorem, we can apply similar techniques as we used for proving the convergence of martingales and we are going to omit the proof here. The convergence property of the reverse martingale will be quite useful in the proof for strong law of large numbers for the U-Statistics in the next section.

\subsection{U-statistics as martingales}
\begin{theorem}
Let $U_n$ be a sequence of U-statistics based on a kernel $\psi$ satisfying $E|\psi(X_1, \dotsc, X_k)|<\infty$. Then $\{U_n\}_{n=k}^\infty$ is a reverse martingale adapted to the $\sigma$-fields $\mathcal{F}_n=\sigma\{U_n, U_{n+1,\dotsc}\}$.
\end{theorem}

\begin{proof}
The $\sigma$-fields $\mathcal{F}_n$ are decreasing clearly. Because of the inherent symmetry of the kernel $\psi$ and that samples $X_1, \dotsc, X_k$ are i.i.d, we can write:
\begin{align*}
    E(\psi (X_{i_1}, \dotsc, X_{i_k})|\mathcal{F}_n)=E(\psi (X_1, \dotsc, X_k)|\mathcal{F}_n)
\end{align*}
for every subset $\{i_1, \dotsc, i_k\}$ of $\{1,2,\dotsc,n\}$ and hence 
\begin{align*}
    U_n=E(U_n|\mathcal{F}_n)&={n\choose k}^{-1}\sum_{(n,k)}E(\psi(X_{i_1}, \dotsc,X_{i_k})|\mathcal{F}_n)
    =E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_n).
\end{align*}
Then we check the integrability of $U_n$
\begin{align*}
E(|U_n|)&=E(|E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_n)|)\le E(E(|\psi(X_1,\dotsc,X_k)||\mathcal{F}_n))
=E(|\psi(X_1,\dotsc, X_k)|)<\infty
\end{align*}

Let $n\ge m$. Then since $\mathcal{F}_n \subseteq \mathcal{F}_m$,
\begin{align*}
    E(U_m|\mathcal{F}_n)&=E\{E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_m)|\mathcal{F}_n\}
    =E\{\psi(X_1,\dotsc,X_k)|\mathcal{F}_n\}
    =U_n.
\end{align*}
\end{proof}

\begin{theorem}\label{mat}
Let $U_n$ be a sequence of U-statistics based on a kernel $\psi$ satisfying $E|\psi(X_1, \dotsc, X_k)|<\infty$, and let $\mathcal{F}_n=\sigma\{X_1, \dotsc, X_n\}$. Then $\{{n \choose c} H_n^{(c)}\}_{n=c}^\infty$ is a martingale adapted to the $\mathcal{F}_n$ for $c=1,\dotsc, k$.
\end{theorem}
\begin{proof}
The $\sigma$-fields $\mathcal{F}_n$ are increasing clearly. It's also easy to check the integrability of random variables $H_n^{(c)}$: Since $h^{(1)}(x_1)=\psi_1(x_1)-\theta$ and $\psi(X_1)$ is integrable, then $h^{(1)}(X_1)$ is integerable. By
\begin{align*}
h^{(c)}(x_1,\dotsc,x_c)=\psi(x_1,\dotsc,x_c)-\sum_{j+1}^{c-1}\sum_{(c,j)}h^{(j)}(x_{i_1},\dotsc,x_{i_j})-\theta
\end{align*}
Recursively we can get $h^{(j)}(x_{i_1},\dotsc,x_{i_j})$ is integrable and $H_n^{(j)}={n\choose j}h^{(j)}(x_{i_1},\dotsc,x_{i_j})$ is also integrable.
Then we have 
\begin{align*}
E\{{{n+1}\choose c} H_{n+1}^{(c)}|\mathcal{F}_n\}=E\{{{n+1}\choose c}{{n+1}\choose c}^{-1}\sum_{(n+1,c)}h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}=
\sum_{(n+1,c)}E\{h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}
\end{align*}

and $E\{h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}=h^{(c)}(X_{i_1},\dotsc,X_{i_c})$ provided all the indices $i_j\le n$ for all $j \in \{i,\dotsc, c\}$. If there exist $l$ indices with $i_j\ge n+1$, Since the inherent symmetry property of the kernal function, without loss of generality we can assume that the first $l$ random variabels are not measurable, then
\begin{align*}
E\{h^{(c)}(X_{i_1},\dotsc,X_{i_l},X_{i_{l+1}},\dotsc, X_{i_c})|\mathcal{F}_n=E\{h^{(c)}(X_{i_1},\dotsc,X_{i_l},x_{i_{l+1}},\dotsc, x_{i_c})\}=h^{(c)}_{c-l}(x_{i_1},\dotsc,x_{i_c})=0
\end{align*}
The last equality holds because of Theorem 2 (i) for the H-decomposition. Hence
\begin{align*}
E\{{{n+1}\choose c} H_{n+1}^{(c)}|\mathcal{F}_n\}=\sum_{(n,c)}h^{(c)}(X_{i_1},\dotsc,X_{i_c})={n\choose c}H_n^{(c)}
\end{align*}
\end{proof}

\section{SLLN for U-statistics}
\begin{theorem}
Suppose $E|\psi(X_1, \dotsc,X_k)|<\infty$. Then $U_n$ converges a.s. to $\theta$.
\end{theorem}
\begin{proof}
Since $U_n$ is a reverse martingale adapted to the 
\end{proof}

\end{document}
