%\documentclass[twocolumn]{article}
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx,algpseudocode}
\usepackage{geometry, amssymb, amsmath, amsthm}
\usepackage{bbm}
\usepackage{enumitem}   
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{chngpage}
\usepackage{breqn}
\usepackage{color,soul}
\usepackage{algorithm}
\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{ref.bib}
\linespread{1.3}
\geometry{margin=1in, headsep=0.25in}

\parindent 0in
\parskip 0.1in

\usepackage[final]{changes}
\setdeletedmarkup{{\color{red}\sout{#1}}}
\usepackage{todonotes}
\newcommand{\kk}[2][]{\todo[color=purple!20,#1]{KK: #2}}

%\usepackage[backend=biber,style=authoryear]{biblatex}
%\bibliography{ref.bib}

% \usepackage[margin=1.0in]{geometry}
% \parskip = 0.1in


%%%%%%%%%%new commands%%%%%%%%%%%%
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\nsum}{{\sum_{i=1}^n}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\V}{{\text{Var}}}
\newcommand{\prob}{{\mathbb{P}}}
\newcommand{\bvar}[1]{\mathbf{#1}} 

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}

\newtheorem{Lemma}{Lemma}

\newtheorem{Def}{Definition}
\numberwithin{Def}{section}
\newtheorem{ex}{Example}
\newtheorem{Prop}{Proposition}
\newtheorem{Cor}[theorem]{Corollary}
\newtheorem{Rmk}{Remark}

%%%%%%%%%%%%%%%%%%%%%%

\title{U-Statistics}
\author{Lily Chou, Kelly Kung, and Zihuan Qiao}
\date{\today}

\begin{document}
\thispagestyle{empty}

\begin{center}
{\LARGE \bf U-Statistics}\\
{\large Lily Chou, Kelly Kung, Zihuan Qiao}\\
May 2, 2019
\end{center}

Throughout this semester in Probability Theory II, we have learned many concepts such as Law of Large Numbers, Central Limit Theorem, and Martingales. U-Statistics is an example from Statistics where many of the Probability concepts can be applied to. We begin the discussion of U-Statistics with some background information.

\section{Introduction}
Before introducing the concept of U-Statistics, we will first define terms and introduce notation that will be used throughout this handout. 
\begin{Def}{}
We denote $\theta$ as a \textit{functional} defined on a set $\mathcal{F}$ of distribution functions on $\R$, i.e. $\theta = \theta(F), F \in \mathcal{F}$.
\end{Def}

\begin{Def}{}
For every sufficiently large $n$, we say that $\theta$ \text{admits an unbiased estimator} if there is a function $f_n(X_1, \dotsc, X_n)$ on $n$ variables such that \begin{align}\label{unbiased} E(f_n(X_1, \dotsc, X_n)) = \theta(F).\end{align}
\end{Def}

In 1946, Paul Halmos (\cite{halmos1946theory,}) asked two questions that prompted the study of U-Statistics:
\begin{enumerate}
    \item Does there exist an estimator of $\theta$ that will be unbiased whatever the distribution function $F$ may be? Can we characterize the sets $\mathcal{F}$ and the functionals $\theta$ for which the answer is yes?
    \item If such an estimator exists, what is it? If several exist, which is the best?
\end{enumerate}

These questions will be answered throughout this handout. In fact, Halmos answers the part of the first question with the following theorem. 
\begin{theorem}
With a sample of random variables $X_1, \dotsc, X_n$, a functional $\theta$ defined on a set $\mathcal{F}$ of distribution functions admits an unbiased estimator if and only if there is a function $\psi$ of k variables such that 
\begin{align}\label{unbiased_est}
    \theta(F) = \int_{-\infty}^{\infty} \dotsc \int_{-\infty}^{\infty} \psi(x_1, x_2, \dotsc, x_k)dF(x_1) \dotsc dF(x_k)
\end{align}
\end{theorem}

\begin{proof}
Note that for $n \geq k$, if we let $f_n(X_1, \dotsc, X_n) = \psi(X_1, \dotsc, X_k)$, we have that Equation $(2)$ satisfies the condition of unbiasedness in Equation $(1)$. Furthermore, letting $n=k$ implies that Equation $(2)$ is equivalent to Equation $(3)$. Thus $(2) \iff (3)$. 
\end{proof}

\begin{Def}
A functional $\theta(F)$ that satisfies Equation (2) for some function $\psi$ with $k$ variables is called a \textit{regular statistical functional of degree k}. Also, the function $\psi$ is called the \textit{kernel} of the functional. 
\end{Def}

In Theorem 1, the kernel function is only evaluated at $k$ random variables, but since we assume that we have independent identically distributed random variables $X_1, \dotsc, X_n$, it intuitively makes sense that we want a symmetric kernel function that is evaluated at all $n$ observations. 

\begin{Def}
A function is \textit{symmetric} if it is invariant under permutations of its arguments. 
\end{Def}

With a symmetric kernel function and a large enough Borel set $E$, we can state the following theorems. First, we state a lemma.

\begin{Lemma}\label{lemA}
Let $\mathcal{F}$ contain all distributions with finite support in E, and let $f$ be a symmetric function of n variables with 
    \begin{align*}
        \int \dotsc \int_{\R_n} f(x_1, \dotsc, x_n) \prod_{i=1}^n dF(x_i) = 0 \text{ for all } F \in \mathcal{F}
    \end{align*}
    Then $f(x_1, \dotsc, x_n) = 0$ whenever $x_i \in E, i = 1, \dotsc, n$. 
\end{Lemma}

\begin{theorem}\label{unique}
Let $\mathcal{F}$ contain all distributions with finite support in $E$, and let $\theta$ be a regular functional satisfying Equation \ref{unbiased_est}. Then up to equality on $E$, there is a unique symmetric unbiased estimator of $\theta$. Here, estimators are identical if they agree on the Borel set $E$.
\end{theorem}
\begin{proof}
Note that the kernel function $\psi(x_1, \dotsc, x_k)$ may not be symmetric, and so we define a symmetric function $\psi^{[n]}(x_1, \dotsc, x_n) = \frac{(n-1)!}{n!}\sum \psi(x_{i_1}, \dotsc, x_{i_k})$ where the sum is taken over all permutations of $\{i_1, \dotsc, i_k\}$ distinct integers from $\{1, \dotsc, n\}$. Since the random variables are i.i.d and $\psi(x_{i_1}, \dotsc, x_{i_k})$ is a kernel function, we know that $\psi^{[n]}$ is unbiased. Consider another symmetric unbiased estimator $f$, and we consider $f - \psi^{[n]}$. By Lemma \ref{lemA}, since both $f$ and $\psi^{[n]}$ are unbiased, we know that $E(f - \psi^{[n]}) = 0$, which implies $f - \psi^{[n]} = 0$, i.e. $f = \psi^{[n]}$. 
\end{proof}

 \begin{theorem}
    Suppose that $E = \R$. Let $\theta$ be a regular functional of degree k defined by Equation \ref{unbiased_est} on a set of $\mathcal{F}$ of distribution functions containing all distributions having finite support. Let $f$ be an unbiased estimate of $\theta$ based on a sample of size n, so that $f$ satisfies Equation \ref{unbiased}. Then $var(f) \geq var(\psi^{[n]})$ for all $F \in \mathcal{F}$. 
    \end{theorem}
    
   \begin{proof}
    Define $f^{[n]}(x_1, \dotsc, x_n) = \frac{1}{n!}\sum_{(n)}f(x_{i_1}, \dotsc, x_{i_n})$ where the sum $\sum_{(n)}$ is taken over all permutations $(i_1, \dotsc, i_n)$ of $\{1, \dotsc, n\}$. Then $f^{[n]}$ is a symmetric unbiased estimator, and so by Theorem \ref{unique} agrees with $\psi^{[n]}$ on $\R$. Since both estimators are unbiased, we just have to show the inequality using second moments. Using the Cauchy-Schwartz Inequality gives $(\psi^{[n]})^2 \leq \frac{1}{n!}\sum_{(n)}f^2(x_{i_1}, \dotsc, x_{i_n})$. Then, taking the expected value of the inequality gives $E((\psi^{[n]})^2) \leq E(f^2(x_{i_1}, \dotsc, x_{i_n}))$. This implies that $var(f) \geq var(\psi^{[n]})$.
    \end{proof}
 
 There is an equivalent theorem with results similar to Theorem 2 and Theorem 3 for when $\mathcal{F}$ contains distribution functions containing all absolutely continuous distribution functions. 
 
    \section{U-Statistics}
    U-statistics are named due to their unbiasedness property, and the name was coined by Hoeffding in 1948 (\cite{hoeffding1948}). U-statistics are of the form 
    \begin{align}\label{u_stat}
        U_n = {n \choose k}^{-1}\sum_{(n,k)}\psi(X_{i_1}, \dotsc, X_{i_k})
    \end{align}

Some examples of U-statistics include:
\begin{ex}\textbf{Sample mean}
Let $\mathcal{F}$ be the set of all distributions whose means exist, such that $\mathcal{F}$ contains all distributions that have finite support on $\R$. The mean functional is given by $\theta(F) = \int_{-\infty}^{\infty} x dF(x)$. Using Equation \ref{u_stat}, we have that $k=1$ and $\psi(X_{i_1}) = x_{i_1}$, which gives $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$.
\end{ex}

\begin{ex}\textbf{Sample variance}
Let $\mathcal{F}$ be the set of all distributions with finite second moment, i.e. $\mathcal{F} = \{F: \int |x|^2 dF(x)<\infty\}$. The variance functional is given by $\theta(F) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2}(x_1 - x_2)^2 dF(x_1)dF(x_2)$. Here, we have that $k=2$ and $\psi(X_{i_1}, X_{i_2}) = \frac{1}{2}(x_{i_1} - x_{i_2})^2$, which gives $s_n^2 = {n \choose 2}^{-1}\sum_{1 \leq i < j \leq n}\frac{1}{2}(X_i - X_j)^2$.
\end{ex}

\begin{ex}\textbf{Kendall's Tau} Here, we consider two points $P_1$ and $P_2$ on a plane. We say that $P_1$ and $P_2$ are concordant if the line joining the points have positive slope, and otherwise, they are discordant. Here, we let $\mathcal{F}$ be the set of distribution functions with all absolutely continuous bivariate random vectors. The functional $\tau = \mathbb{P}(P_1, P_2 \text{ concordant}) - \mathbb{P}(P_1, P_2 \text{ discordant})$. We define the kernel $t$ as 
\begin{align*}
    t(P_1, P_2) = \begin{cases} 1 & \text{if $P_1$ and $P_2$ are concordant} \\ -1 & \text{if $P_1$ and $P_2$ are discordant} = sgn(X_1 - X_2)(Y_1 - Y_2).
    \end{cases}
\end{align*}
Note that $E(t(P_1, P_2)) = \mathbb{P}(P_1, P_2 \text{ concordant}) - \mathbb{P}(P_1, P_2 \text{ discordant})$, which verifies that $t(P_1, P_2)$ is indeed a kernel. Also, here, we have $k=2$. Using Equation \ref{u_stat}, we have that the estimator of Kendall's tau is $\tau_n = {n \choose 2}\sum_{(n,2)}t(P_i, P_j)$.
\end{ex}

\subsection{Variance of U-Statistics}
In the case of i.i.d random variables, we can express the variance of U-statistics in terms of conditional expectations. 

\begin{Def}
Given $c = 1, \dotsc, k$, we denote \begin{align}
    \psi_c(x_1, \dotsc, x_c) = E(\psi(x_1, \dotsc, x_c, X_{c+1}, \dotsc X_k)) = E(\psi(X_1, \dotsc, X_k)|X_1 = x_1, \dotsc, X_c = x_c)
\end{align}
as the \textit{conditional expectation}. We also denote the variance of the conditional expectation as 
\begin{align}
    \sigma_c^2 = var(\psi_c(X_1, \dotsc, X_c))
\end{align}
where $\sigma_0^2 = 0$. 
\end{Def}

\begin{theorem}\label{cond_exp}
The functions $\psi_c$ defined above have the properties
        \begin{enumerate}[label = (\roman*)]
            \item $\psi_c(x_1, \dotsc, x_c) = E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)), \text{ for } 1 \leq c <d \leq k$
            \item $E(\psi_c(X_1, \dotsc, X_c)) = E(\psi(X_1, \dotsc, X_k))$
        \end{enumerate}
\end{theorem}
\begin{proof}
To prove Theorem 4, we rely on the Tower Property. Part (i) is given by 
\begin{align*}
 E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)) &= E(E(\psi(X_1, \dotsc, X_k)|x_1, \dotsc, x_d)|x_1, \dotsc, x_c)  \\
 &= E(\psi(X_1, \dotsc, X_k)|x_1, \dotsc, x_c)  &&\text{[since $c<d$]}\\
 &=\psi_c(x_1, \dotsc, x_c)
\end{align*}

Part (ii) is also given be the Tower Property, where we have 
\begin{align*}
    E(\psi_c(x_1, \dotsc, x_c)) &= E(E(\psi(X_1, \dotsc, X_k|x_1, \dotsc, x_c)) = E(\psi(X_1, \dotsc, X_k)
\end{align*}
\end{proof}

\begin{theorem}
We can express the variance of the conditional expectation as \begin{align}
    \sigma_c^2 = cov(\psi(S_1), \psi(S_2))
\end{align}
where $S_1, S_2$ are two subsets of size $k$ from $\{1, \dotsc, n\}$ with $c$ elements in common. 
\end{theorem}
\end{document}