%\documentclass[twocolumn]{article}
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx,algpseudocode}
\usepackage{geometry, amssymb, amsmath, amsthm}
\usepackage{bbm}
\usepackage{enumitem}   
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{chngpage}
\usepackage{breqn}
\usepackage{color,soul}
\usepackage{algorithm}
\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{ref.bib}
\linespread{1.3}
\geometry{margin=1in, headsep=0.25in}

\parindent 0in
\parskip 0.1in

\usepackage[final]{changes}
\setdeletedmarkup{{\color{red}\sout{#1}}}
\usepackage{todonotes}
\newcommand{\kk}[2][]{\todo[color=purple!20,#1]{KK: #2}}

%\usepackage[backend=biber,style=authoryear]{biblatex}
%\bibliography{ref.bib}

% \usepackage[margin=1.0in]{geometry}
% \parskip = 0.1in


%%%%%%%%%%new commands%%%%%%%%%%%%
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\nsum}{{\sum_{i=1}^n}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\V}{{\text{Var}}}
\newcommand{\prob}{{\mathbb{P}}}
\newcommand{\bvar}[1]{\mathbf{#1}} 

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}

\newtheorem{Lemma}{Lemma}

\newtheorem{Def}{Definition}
\numberwithin{Def}{section}
\newtheorem{ex}{Example}
\newtheorem{Prop}{Proposition}
\newtheorem{Cor}[theorem]{Corollary}
\newtheorem{Rmk}{Remark}

%%%%%%%%%%%%%%%%%%%%%%

\title{U-Statistics}
\author{Lily Chou, Kelly Kung, and Zihuan Qiao}
\date{\today}

\begin{document}
\thispagestyle{empty}

\begin{center}
{\LARGE \bf U-Statistics}\\
{\large Lily Chou, Kelly Kung, Zihuan Qiao}\\
May 2, 2019
\end{center}

Throughout this semester in Probability Theory II, we have learned many concepts such as Law of Large Numbers, Central Limit Theorem, and Martingales. U-Statistics is an example from Statistics where many of the Probability concepts can be applied to. We begin the discussion of U-Statistics with some background information.

\section{Introduction}
Before introducing the concept of U-Statistics, we will first define terms and introduce notation that will be used throughout this handout. 
\begin{Def}{}
We denote $\theta$ as a \textit{functional} defined on a set $\mathcal{F}$ of distribution functions on $\R$, i.e. $\theta = \theta(F), F \in \mathcal{F}$.
\end{Def}

\begin{Def}{}
For every sufficiently large $n$, we say that $\theta$ \text{admits an unbiased estimator} if there is a function $f_n(X_1, \dotsc, X_n)$ on $n$ variables such that \begin{align}\label{unbiased} E(f_n(X_1, \dotsc, X_n)) = \theta(F).\end{align}
\end{Def}

In 1946, Paul Halmos (\cite{halmos1946theory,}) asked two questions that prompted the study of U-Statistics:
\begin{enumerate}
    \item Does there exist an estimator of $\theta$ that will be unbiased whatever the distribution function $F$ may be? Can we characterize the sets $\mathcal{F}$ and the functionals $\theta$ for which the answer is yes?
    \item If such an estimator exists, what is it? If several exist, which is the best?
\end{enumerate}

These questions will be answered throughout this handout. In fact, Halmos answers the part of the first question with the following theorem. 
\begin{theorem}
With a sample of random variables $X_1, \dotsc, X_n$, a functional $\theta$ defined on a set $\mathcal{F}$ of distribution functions admits an unbiased estimator if and only if there is a function $\psi$ of k variables such that 
\begin{align}\label{unbiased_est}
    \theta(F) = \int_{-\infty}^{\infty} \dotsc \int_{-\infty}^{\infty} \psi(x_1, x_2, \dotsc, x_k)dF(x_1) \dotsc dF(x_k)
\end{align}
\end{theorem}

\begin{proof}
Note that for $n \geq k$, if we let $f_n(X_1, \dotsc, X_n) = \psi(X_1, \dotsc, X_k)$, we have that Equation $(2)$ satisfies the condition of unbiasedness in Equation $(1)$. Furthermore, letting $n=k$ implies that Equation $(2)$ is equivalent to Equation $(3)$. Thus $(2) \iff (3)$. 
\end{proof}

\begin{Def}
A functional $\theta(F)$ that satisfies Equation (2) for some function $\psi$ with $k$ variables is called a \textit{regular statistical functional of degree k}. Also, the function $\psi$ is called the \textit{kernel} of the functional. 
\end{Def}

In Theorem 1, the kernel function is only evaluated at $k$ random variables, but since we assume that we have independent identically distributed random variables $X_1, \dotsc, X_n$, it intuitively makes sense that we want a symmetric kernel function that is evaluated at all $n$ observations. 

\begin{Def}
A function is \textit{symmetric} if it is invariant under permutations of its arguments. 
\end{Def}

With a symmetric kernel function and a large enough Borel set $E$, we can state the following theorems. First, we state a lemma.

\begin{Lemma}\label{lemA}
Let $\mathcal{F}$ contain all distributions with finite support in E, and let $f$ be a symmetric function of n variables with 
    \begin{align*}
        \int \dotsc \int_{\R_n} f(x_1, \dotsc, x_n) \prod_{i=1}^n dF(x_i) = 0 \text{ for all } F \in \mathcal{F}
    \end{align*}
    Then $f(x_1, \dotsc, x_n) = 0$ whenever $x_i \in E, i = 1, \dotsc, n$. 
\end{Lemma}

\begin{theorem}\label{unique}
Let $\mathcal{F}$ contain all distributions with finite support in $E$, and let $\theta$ be a regular functional satisfying Equation \ref{unbiased_est}. Then up to equality on $E$, there is a unique symmetric unbiased estimator of $\theta$. Here, estimators are identical if they agree on the Borel set $E$.
\end{theorem}
\begin{proof}
Note that the kernel function $\psi(x_1, \dotsc, x_k)$ may not be symmetric, and so we define a symmetric function $\psi^{[n]}(x_1, \dotsc, x_n) = \frac{(n-1)!}{n!}\sum \psi(x_{i_1}, \dotsc, x_{i_k})$ where the sum is taken over all permutations of $\{i_1, \dotsc, i_k\}$ distinct integers from $\{1, \dotsc, n\}$. Since the random variables are i.i.d and $\psi(x_{i_1}, \dotsc, x_{i_k})$ is a kernel function, we know that $\psi^{[n]}$ is unbiased. Consider another symmetric unbiased estimator $f$, and we consider $f - \psi^{[n]}$. By Lemma \ref{lemA}, since both $f$ and $\psi^{[n]}$ are unbiased, we know that $E(f - \psi^{[n]}) = 0$, which implies $f - \psi^{[n]} = 0$, i.e. $f = \psi^{[n]}$. 
\end{proof}

 \begin{theorem}
    Suppose that $E = \R$. Let $\theta$ be a regular functional of degree k defined by Equation \ref{unbiased_est} on a set of $\mathcal{F}$ of distribution functions containing all distributions having finite support. Let $f$ be an unbiased estimate of $\theta$ based on a sample of size n, so that $f$ satisfies Equation \ref{unbiased}. Then $var(f) \geq var(\psi^{[n]})$ for all $F \in \mathcal{F}$. 
    \end{theorem}
    
   \begin{proof}
    Define $f^{[n]}(x_1, \dotsc, x_n) = \frac{1}{n!}\sum_{(n)}f(x_{i_1}, \dotsc, x_{i_n})$ where the sum $\sum_{(n)}$ is taken over all permutations $(i_1, \dotsc, i_n)$ of $\{1, \dotsc, n\}$. Then $f^{[n]}$ is a symmetric unbiased estimator, and so by Theorem \ref{unique} agrees with $\psi^{[n]}$ on $\R$. Since both estimators are unbiased, we just have to show the inequality using second moments. Using the Cauchy-Schwartz Inequality gives $(\psi^{[n]})^2 \leq \frac{1}{n!}\sum_{(n)}f^2(x_{i_1}, \dotsc, x_{i_n})$. Then, taking the expected value of the inequality gives $E((\psi^{[n]})^2) \leq E(f^2(x_{i_1}, \dotsc, x_{i_n}))$. This implies that $var(f) \geq var(\psi^{[n]})$.
    \end{proof}
 
 \begin{Rmk}
 There is an equivalent theorem with results similar to Theorem 2 and Theorem 3 for when $\mathcal{F}$ contains distribution functions containing all absolutely continuous distribution functions. 
 \end{Rmk}
 
    \section{U-Statistics}
    U-statistics are named due to their unbiasedness property, and the name was coined by Hoeffding in 1948 (\cite{hoeffding1948}). U-statistics are of the form 
    \begin{align}\label{u_stat}
        U_n = {n \choose k}^{-1}\sum_{(n,k)}\psi(X_{i_1}, \dotsc, X_{i_k})
    \end{align}

Some examples of U-statistics include:
\begin{ex}\textbf{Sample mean}
Let $\mathcal{F}$ be the set of all distributions whose means exist, such that $\mathcal{F}$ contains all distributions that have finite support on $\R$. The mean functional is given by $\theta(F) = \int_{-\infty}^{\infty} x dF(x)$. Using Equation \ref{u_stat}, we have that $k=1$ and $\psi(X_{i_1}) = x_{i_1}$, which gives $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$.
\end{ex}

\begin{ex}\textbf{Sample variance}
Let $\mathcal{F}$ be the set of all distributions with finite second moment, i.e. $\mathcal{F} = \{F: \int |x|^2 dF(x)<\infty\}$. The variance functional is given by $\theta(F) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2}(x_1 - x_2)^2 dF(x_1)dF(x_2)$. Here, we have that $k=2$ and $\psi(X_{i_1}, X_{i_2}) = \frac{1}{2}(x_{i_1} - x_{i_2})^2$, which gives $s_n^2 = {n \choose 2}^{-1}\sum_{1 \leq i < j \leq n}\frac{1}{2}(X_i - X_j)^2$.
\end{ex}

\begin{ex}\textbf{Kendall's Tau} Here, we consider two points $P_1$ and $P_2$ on a plane. We say that $P_1$ and $P_2$ are concordant if the line joining the points have positive slope, and otherwise, they are discordant. Here, we let $\mathcal{F}$ be the set of distribution functions with all absolutely continuous bivariate random vectors. The functional $\tau = \mathbb{P}(P_1, P_2 \text{ concordant}) - \mathbb{P}(P_1, P_2 \text{ discordant})$. We define the kernel $t$ as 
\begin{align*}
    t(P_1, P_2) = \begin{cases} 1 & \text{if $P_1$ and $P_2$ are concordant} \\ -1 & \text{if $P_1$ and $P_2$ are discordant} = sgn(X_1 - X_2)(Y_1 - Y_2).
    \end{cases}
\end{align*}
Note that $E(t(P_1, P_2)) = \mathbb{P}(P_1, P_2 \text{ concordant}) - \mathbb{P}(P_1, P_2 \text{ discordant})$, which verifies that $t(P_1, P_2)$ is indeed a kernel. Also, here, we have $k=2$. Using Equation \ref{u_stat}, we have that the estimator of Kendall's tau is $\tau_n = {n \choose 2}\sum_{(n,2)}t(P_i, P_j)$.
\end{ex}

\subsection{Variance of U-Statistics}
In the case of i.i.d random variables, we can express the variance of U-statistics in terms of conditional expectations. 

\begin{Def}
Given $c = 1, \dotsc, k$, we denote \begin{align}
    \psi_c(x_1, \dotsc, x_c) = E(\psi(x_1, \dotsc, x_c, X_{c+1}, \dotsc X_k)) = E(\psi(X_1, \dotsc, X_k)|X_1 = x_1, \dotsc, X_c = x_c)
\end{align}
as the \textit{conditional expectation}. We also denote the variance of the conditional expectation as 
\begin{align}
    \sigma_c^2 = var(\psi_c(X_1, \dotsc, X_c))
\end{align}
where $\sigma_0^2 = 0$. 
\end{Def}

\begin{theorem}\label{cond_exp}
The functions $\psi_c$ defined above have the properties
        \begin{enumerate}[label = (\roman*)]
            \item $\psi_c(x_1, \dotsc, x_c) = E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)), \text{ for } 1 \leq c <d \leq k$
            \item $E(\psi_c(X_1, \dotsc, X_c)) = E(\psi(X_1, \dotsc, X_k))$
        \end{enumerate}
\end{theorem}
\begin{proof}
To prove Theorem \ref{cond_exp}, we rely on the Tower Property. Part (i) is given by 
\begin{align*}
 E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)) &= E(E(\psi(X_1, \dotsc, X_k)|x_1, \dotsc, x_d)|x_1, \dotsc, x_c)  \\
 &= E(\psi(X_1, \dotsc, X_k)|x_1, \dotsc, x_c)  &&\text{[since $c<d$]}\\
 &=\psi_c(x_1, \dotsc, x_c)
\end{align*}

Part (ii) is also given be the Tower Property, where we have 
\begin{align*}
    E(\psi_c(x_1, \dotsc, x_c)) &= E(E(\psi(X_1, \dotsc, X_k|x_1, \dotsc, x_c)) = E(\psi(X_1, \dotsc, X_k)
\end{align*}
\end{proof}

\begin{theorem}\label{cond_exp_prop}
We can express the variance of the conditional expectation as \begin{align}
    \sigma_c^2 = cov(\psi(S_1), \psi(S_2))
\end{align}
where $S_1, S_2$ are two subsets of size $k$ from $\{1, \dotsc, n\}$ with $c$ elements in common. 
\end{theorem}

\begin{proof}
By Theorem \ref{cond_exp} part (ii), we know that $E(\psi_c(X_1, \dotsc, X_c)) = E(\psi(X_1, \dotsc, X_k))$, so we just have to show $E(\psi(X_1, \dotsc, X_k)\psi(X_1, \dotsc, X_c, X_{k+1}, \dotsc, X_{2k-c})) = E\left[(\psi_c(X_1, \dotsc, X_c))^2\right]$. We thus have the following:
\begin{fontsize}{8}{12}
\begin{align*}
    E(\psi(X_1, \dotsc, X_k)\psi(X_1, \dotsc, X_c, X_{k+1}, \dotsc, X_{2k-c}))&= E\left[E(\psi(X_1, \dotsc, X_k)\psi(X_1, \dotsc, X_c, X_{k+1}, \dotsc, X_{2k-c})|X_1, \dotsc, X_c)\right]\\
    &=E\left[E(\psi(X_1, \dotsc, X_k)|X_1, \dotsc, X_c) E(\psi(X_1, \dotsc, X_c, X_{k+1}, \dotsc, X_{2k-c})|X_1, \dotsc, X_c)\right]\\
    &=E\left[\psi_c(X_1, \dotsc, X_c)^2\right]
\end{align*}
\end{fontsize}Thus, we have $cov(\psi(S_1)\psi(S_2)) = E\left[\psi_c(X_1, \dotsc, X_c)^2\right] - E\left[\psi_c(X_1, \dotsc, X_c)\right]^2 = \sigma_c^2$.
\end{proof}

Using the results from Theorem \ref{cond_exp_prop}, we can derive the variance for the U-statistic.

\begin{theorem}\label{var_u_stat}
Let $U_n$ be a U-statistic with kernel $\psi$ of degree k. Then, 
\begin{align}
    var(U_n) = {n \choose k}^{-1}\sum_{c=1}^k{k \choose c}{n-k \choose k-c} \sigma_c^2
\end{align}
\end{theorem}

\begin{proof}
    \begin{align*}
        var(U_n) &= var({n \choose k}^{-1} \sum_{(n,k)}\psi(X_{i_1}, \dotsc, X_{i_n})) = cov({n \choose k}^{-1} \sum_{(n,k)}\psi(X_{i_1}, \dotsc, X_{i_n}), {n \choose k}^{-1} \sum_{(n,k)}\psi(X_{j_1}, \dotsc, X_{j_n}))\\
        &= {n \choose k}^{-2}\sum_{(n,k)}\sum_{(n,k)}cov(\psi(X_{i_1}, \dotsc, X_{i_n}), \psi(X_{j_1}, \dotsc, X_{j_n})) ={n \choose k}^{-1}\sum_{c=1}^k {k \choose c}{n-k \choose k-c}\sigma_c^2
    \end{align*}
by using the results in Theorem \ref{cond_exp_prop}. The coefficients come from counting the number of ways of choosing two subsets of size $k$ with $c$ elements in common. 
\end{proof}

\subsection{Covariance of U-statistics}
Suppose now we have two U-statistics $U_n^{(1)}$ and $U_n^{(2)}$ where they are based on the same sample $X_1, \dotsc, X_n$, but they have different kernel functions, $\psi$ and $\phi$ of degrees $k_1$ and $k_2$, respectively. Assume that WLOG, $k_1 \leq k_2$. 

We first define $\sigma_{c,d}^2 = cov(\psi_c(X_1, \dotsc, X_c), \phi_d(X_1, \dotsc, X_d)$. Using this, we can define the covariance of U-statistics. 
\begin{theorem}
Suppose that $c \leq d$. If $S_1$ is in $S_{n,k_1}$ and $S_2$ in $S_{n, k_2}$ with $|S_1 \cap S_2| = c$, then 
    \begin{align}
    \sigma_{c,d}^2 = cov(\psi(S_1), \phi(S_2))
    \end{align}
\end{theorem}

\begin{proof}
    The proof is very similar to that of the Theorem \ref{cond_exp_prop} and uses the result from Theorem \ref{cond_exp}. However, in the product term, we condition on $d$ terms instead of $c$ terms. 
\end{proof}

\begin{Rmk}
 As a consequence, we have $\sigma_{c,c}^2 = \sigma_{c,c+1}^2 = \dotsc = \sigma_{c,k_2}^2$ for $c = 1, \dotsc, k_1$.    
\end{Rmk}
    
    \begin{theorem}
    Let $U_n^{(1)}$ and $U_n^{(1)}$ be defined as above. Then, 
    \begin{align}
        cov(U_n^{(1)}, U_n^{(2)}) = {n \choose k_1}^{-1}\sum_{c=1}^{k_1}{k_2 \choose c}{n-k_2 \choose k_1 - c}\sigma_{c,c}^2
        \end{align}
    \end{theorem}
    \begin{proof}
    \begin{align*}
      cov(U_n^{(1)}, U_n^{(2)}) &= {n \choose k_1}^{-1}{n \choose k_2}^{-1}\sum_{(n,k_1)}\sum_{(n, k_2)} cov(\psi(S_1), \phi(S_2)) = {n \choose k_1}^{-1}{n \choose k_2}^{-1}\sum_{c=1}^{k_1}\sum_{|S_1 \cap S_2| = c} cov(\psi(S_1), \phi(S_2)) \\
      &= {n \choose k_1}^{-1}\sum_{c=1}^{k_1}{k_2 \choose c}{n-k_2 \choose k_1 - c}\sigma_{c,c}^2
    \end{align*}
since there are ${n \choose k_2}{k_2 \choose c}{n-k_2 \choose k_1 - c}$ ways to choose two subsets $S_1, S_2$ with $k_1$ and $k_2$ elements from the sample, where $|S_1 \cap S_2| = c$.
    \end{proof}
    
\subsection{H-Decomposition}
U-statistics of degree $k$ can be decomposed in terms of a sum of uncorrelated U-statistics of degree $1, 2, \dotsc, 2$ that has variances of decreasing order in $n$ (\cite{hoeffding1961strong}). Using this decomposition, it is easier to prove some of the  results discussed later on in this handout. 

We first introduce kernels $h^{(1)}, \dotsc, h^{(k)}$ of degrees $1, \dotsc, k$ that are defined recursively: 
\begin{align}
    h^{(1)}(x_1) &= \psi_1(x_1) - \theta \notag \\
    h^{(c)}(x_1, \dotsc, x_c) &= \psi_c(x_1, \dotsc, x_c) - \sum_{j=1}^{c-1}\sum_{(c,j)}h^{(j)}(x_{i_1}, \dotsc, x_{i_j}) - \theta \label{h_recursive}
\end{align}
where $c = 2, \dotsc, k$.

\begin{theorem}
For $j = 1, \dotsc, k$, let $H_n^{(j)}$ be the U-statistic based on the kernel $h^{(j)}$, where \begin{align}
H_n^{(j)} = {n \choose j}^{-1} \sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j}). 
\end{align}
Then,
\begin{align}
    U_n  = \theta + \sum_{j=1}^k{k \choose j}H_n^{(j)}
\end{align}
\end{theorem}
\begin{proof}
First, let $S_j\{i_1, \dotsc, i_k\}$ denote the sum $\sum h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j})$, which is summed over all $j$-subsets $\{\nu_1, \dotsc, \nu_j\}$ of $\{1, \dotsc, n\}$. Then, we have the relationship $\sum_{(n,k)}S_j\{i_1, \dotsc, i_k\} = {n-j \choose k-j}\sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j})$, which holds due to the identity ${n-j \choose k-j}{n \choose j} = {n \choose k}{k \choose j} \implies {n \choose k}^{-1}{n-j \choose k-j} = {k \choose j}{n \choose j}^{-1}$. We can then decompose $U_n$ as follows.
\begin{align*}
    U_n &= {n \choose k}^{-1}\sum_{(n,k)}\psi(x_{i_1}, \dotsc, x_{i_k}) = {n \choose k}^{-1}\sum_{(n,k)}\left[\sum_{j=1}^kS_j\{i_1, \dotsc, i_k\} + \theta \right] &&\text{[from Eq. \ref{h_recursive} with $c = k$]} \\
    &= \theta + {n \choose k}^{-1}\sum_{j=1}^n{n-j \choose k-j}\sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j}) = \theta + {n \choose k}^{-1}\sum_{j=1}^n{k \choose j}H_n^{(j)}
\end{align*}
\end{proof}

\begin{Rmk}
We can also truncate the H-decomposition up to $c$ terms, and we are then left with a remainder term $R_n^{(c)}$, i.e. 
\begin{align}
    U_n = \theta + \sum_{j=1}^c{k \choose j} H_n^{(c)} + R_n^{(c)}
\end{align}
where $R_n^{(c)}$ is a U-statistic with kernel $\sum_{j=c+1}^k S_j\{1, \dotsc, k\}$.  
\end{Rmk}

\begin{Rmk}
We can represent the functions $h^{(c)}$ in a slightly different manner. First, let $G_x$ denote the distribution function of a single point mass at $x$. Then, we have 
\begin{align}
    h^{(1)}(x_1) - \psi(x_1) - \theta  &= \int \dotsc \int \psi(x_1, u_2, \dotsc, d_k)\prod_{i=2}^k dF(u_i) - \int \dotsc \int \psi(u_1, u_2, \dotsc, d_k)\prod_{i=1}^k dF(u_i) \notag \\
    &= \int \dotsc \int \psi(u_1, \dotsc, u_k)(dG_{x_1}(u_1) - dF(u_1))\prod_{i=2}^k dF(u_i) \notag \\
    h^{(j)}(x_1, \dotsc, x_j) &= \int \dotsc \int \psi(u_1, \dotsc, u_k) \prod_{i=1}^j(dG_{x_i}(u_i) - dF(u_i))\prod_{j+1}^kdF(u_i) \label{h_func_rep}
\end{align}
Here, the relation in Equation \ref{h_func_rep} is derived by taking the product of $dG_{x_i}(u_i)$. In particular, we have $\prod_{i=1}^kdG_{x_i}(u_i) = \prod_{i=1}^k\left[(dG_{x_i}(u_i) - dF(u_i)) + dF(u_i) \right]$, which leads to $$\prod_{i=1}^kdG_{x_i}(u_i) = \sum_{c=0}^k \sum_{(k,c)}\prod_{j=1}^c\left(dG_{x_{i_j}}(u_{i_j}) - dF(u_{i_j})\right)\prod_{c+1}^k dF(u_{i'_j}).$$
\end{Rmk}

\end{document}