\documentclass{beamer}

% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{Madrid}       % or try default, Darmstadt, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{serif}    % or try default, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb,scrextend, amsfonts}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{ref.bib}
% On Overleaf, these lines give you sharper preview images.
% You might want to `comment them out before you export, though.
\usepackage{pgfpages}
\pgfpagesuselayout{resize to}[%
  physical paper width=8in, physical paper height=6in]

% Here's where the presentation starts, with the info for the title slide
\title{U-Statistics}
\author{Lily Chou, Kelly Kung, Zihuan Qiao}
\institute{Probability Theory II}
\date{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% These three lines create an automatically generated table of contents.
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}{Background Information}
    \begin{block}{Functional}
    We define a functional, $\theta$, defined on a set $\mathcal{F}$ as 
    \begin{align}
        \theta = \theta(F), F \in \mathcal{F}
    \end{align}
    \end{block}
    \begin{block}{Unbiasedness}
    We say that a functional $\theta$ admits an unbiased estimator if for a sufficiently large sample of independent random variables $X_1, \dotsc, X_n$, there exists a function $f_n(X_1, \dotsc, X_n)$ such that 
    \begin{align}\label{unbiased}
        E(f_n(X_1, \dotsc, X_n)) = \theta(F), \forall F \in \mathcal{F}
    \end{align}
    \end{block}
\end{frame}
\begin{frame}{Motivation}
The theory of U-statistics started with Paul Richard Halmos (\cite{halmos1946theory}) when he proposed two questions:
\begin{enumerate}
        \item Does there exist an estimator of $\theta = \theta(F)$ that is unbiased, no matter what $F$ is. Can we characterize the sets $\mathcal{F}$ and $\theta$ for which the answer is yes.
        
        \item What is this estimator? If several exist, which is the best estimator?
\end{enumerate}
\end{frame}

\begin{frame}{Characterization of Functionals with Unbiased Estimators}

  \begin{theorem}
    A functional $\theta$ defined on a set $\mathcal{F}$ of distribution functions admits an unbiased estimator if and only if there is a function $\psi$ of k variables such that 
    \begin{align}\label{unbiased est}
        \theta(F) = \int_{-\infty}^{\infty} \dotsc \int_{-\infty}^{\infty} \psi(x_1, x_2, \dotsc, x_k)dF(x_1) \dotsc dF(x_k)
    \end{align}
    for all F in $\mathcal{F}$.
    \end{theorem}
    Here, $\theta$ is called a \textbf{regular statistical functional} of degree k and $\psi$ is called \textbf{the kernel} of the functional. 
 
\end{frame}

\begin{frame}{Definition}
U-Statistics are first coined by Hoeffding (\cite{hoeffding1992class}) are named due to their unbiasedness
\begin{block}{Definition:}
Statistics that have the form
        \begin{align}
        \hat{\theta} = {n \choose k}^{-1} \sum_{(n,k)} \psi^{[k]}(X_{i_1}, \dotsc, X_{i_k})
    \end{align} 
have desirable properties as estimators of regular functionals. These statistics are known as U-statistics. 
\end{block}
\end{frame}
\begin{frame}{Examples}
\begin{itemize}
    \item \textbf{Sample Mean:} $\mathcal{F}$ is set of all distributions whose means exist, i.e. it contains all distributions having finite support on $\mathbb{R}$. The functional is $\theta(F) = \int_{-\infty}^{\infty} x dF(x)$. The U-statistic is $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$
    \item \textbf{Sample Variance:} $\mathcal{F}$ is set of all distributions with finite second moment, i.e. $\mathcal{F} =\{F: \int|x|^2 dF(x)<\infty\}$. The variance functional is given as $var(F) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2}(x_1 - x_2)^2dF(x_1)dF(x_2)$ which is estimated by $s_n^2 = {n \choose 2}^{-1}\sum_{1\leq i <j \leq n}\frac{1}{2}(X_i - X_j)^2$
\end{itemize}
\end{frame}
\begin{frame}{Properties}
\begin{itemize}
    \item For $c = 1, \dotsc, k$, let \begin{align}
            \psi_c(x_1, \dotsc, x_c) = E(\psi(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_k))
        \end{align}
        be the conditional expectations \begin{align}
        \sigma_c^2 = var(\psi_c(X_1, \dotsc, X_c))
        \end{align} 
        be the variances of the conditional expectations.
        Here, the conditional expectation is dependent on the first $c$ random variables. Also define $\sigma_0^2 = 0$. 
        \item The variance of the U-statistics is given as 
    \begin{theorem}
    Let $U_n$ be a U-statistic with a kernel $\psi$ of degree k. Then, 
    \begin{align}
        var(U_n) = {n \choose k}^{-1}\sum_{c=1}^k{k \choose c}{n-k \choose k-c} \sigma_c^2
    \end{align}
    \end{theorem}
    
\end{itemize}
\end{frame}

\begin{frame}{Properties (continued)}

\end{frame}

\section{Strong Law of Large Numbers}
\begin{frame}{SLLN}
\end{frame}
\section{Central Limit Theorem}
\begin{frame}{CLT}
\end{frame}
\begin{frame}{Comparison to our CLT}
\end{frame}
\section{U-Statistics as Martingales}
\begin{frame}{U-Statistics as Martingales}
\end{frame}
\section{Conclusion}
\begin{frame}{Conclusion/Summary}
\end{frame}
\begin{frame}{References}
\printbibliography
\end{frame}

\end{document}