%\documentclass{beamer}
\documentclass[handout]{beamer}

% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{Madrid}       % or try default, Darmstadt, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{serif}    % or try default, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{theorems}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb,scrextend, amsfonts}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage[backend=biber,style=authoryear]{biblatex}

\addbibresource{ref.bib}
% On Overleaf, these lines give you sharper preview images.
% You might want to `comment them out before you export, though.
\usepackage{pgfpages}
\pgfpagesuselayout{resize to}[%
  physical paper width=8in, physical paper height=6in]

\newcommand{\kk}[2][]{\todo[color=purple!20,#1]{KK: #2}}

%%%%%%%%%%new commands%%%%%%%%%%%%
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\nsum}{{\sum_{i=1}^n}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\V}{{\text{Var}}}
\newcommand{\prob}{{\mathbb{P}}}
\newcommand{\bvar}[1]{\mathbf{#1}} 

\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem{Lemma}{Lemma}
\newtheorem{Def}{Definition}
\numberwithin{Def}{section}
\newtheorem{ex}{Example}
\newtheorem{Prop}{Proposition}
\newtheorem{Cor}[theorem]{Corollary}
\newtheorem{Rmk}{Remark}
% Here's where the presentation starts, with the info for the title slide
\title{U-statistics}
\author{Lily Chou, Kelly Kung, Zihuan Qiao}
\institute{Probability Theory II}
\date{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Motivation}
 \begin{itemize}
     \item Important in Estimation Theory as unbiasedness is generally, but not always, a requirement for ``good'' estimator
     \item Theory can be applied to any problems that require estimation of a parameter 
 \end{itemize}   
\end{frame}
% These three lines create an automatically generated table of contents.
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}{Background Information}
\begin{Def}{}
We denote $\theta$ as a \textit{functional} defined on a set $\mathcal{F}$ of distribution functions on $\R$, i.e. $\theta = \theta(F), F \in \mathcal{F}$.
\end{Def}

\begin{Def}{}
For every sufficiently large $n$, we say that $\theta$ \textit{admits an unbiased estimator} if there is a function $f_n(X_1, \dotsc, X_n)$ on $n$ variables such that \begin{align}\label{unbiased} E(f_n(X_1, \dotsc, X_n)) = \theta(F).\end{align}
\end{Def}
\end{frame}
% \begin{frame}{Background Information}
% The theory of U-statistics started with Paul Richard Halmos (\cite{halmos1946theory}) when he proposed two questions:
% \begin{enumerate}
% \pause
%     \item Does there exist an estimator of $\theta$ that will be unbiased whatever the distribution function $F$ may be? Can we characterize the sets $\mathcal{F}$ and the functionals $\theta$ for which the answer is yes?
% \pause
%     \item If such an estimator exists, what is it? If several exist, which is the best?
% \end{enumerate}
% \end{frame}

\begin{frame}{Characterization of $\theta$}

  \begin{theorem}
(\cite{halmos1946theory}) With a sample of random variables $X_1, \dotsc, X_n$, a functional $\theta$ defined on a set $\mathcal{F}$ of distribution functions admits an unbiased estimator if and only if there is a function $\psi$ of k variables such that 
\begin{align}\label{unbiased_est}
    \theta(F) = \int_{-\infty}^{\infty} \dotsc \int_{-\infty}^{\infty} \psi(x_1, x_2, \dotsc, x_k)dF(x_1) \dotsc dF(x_k)
\end{align}
for all $F \in \mathcal{F}$.
\end{theorem}
    Here, $\theta$ is called a \textit{regular statistical functional of degree k} and $\psi$ is called \textit{the kernel} of the functional. 
 
\end{frame}

\begin{frame}{Characterization of $\mathcal{F}$}
\begin{Def}
A function is \textit{symmetric} if it is invariant under permutations of its arguments. 
\end{Def}

\pause
\begin{Lemma}\label{lemA}
Let $\mathcal{F}$ contain all distributions with finite support in E (some Borel set), and let $f$ be a symmetric function of n variables with 
    \begin{align*}
        \int \dotsc \int_{\R_n} f(x_1, \dotsc, x_n) \prod_{i=1}^n dF(x_i) = 0 \text{ for all } F \in \mathcal{F}
    \end{align*}
    Then $f(x_1, \dotsc, x_n) = 0$ whenever $x_i \in E, i = 1, \dotsc, n$. 
\end{Lemma}
\end{frame}

\begin{frame}{Characterization of $\mathcal{F}$}

\begin{theorem}\label{unique}
Let $\mathcal{F}$ contain all distributions with finite support in $E$, and let $\theta$ be a regular functional satisfying Equation \ref{unbiased_est}. Then up to equality on $E$, there is a unique symmetric unbiased estimator of $\theta$.
\end{theorem}
\pause 
\begin{proof}
\begin{itemize}
    \item Define a symmetric function $\psi^{[n]}(x_1, \dotsc, x_n) = \frac{(n-1)!}{n!}\sum_{(n)} \psi(x_{i_1}, \dotsc, x_{i_k})$.
    \item $\psi^{[n]}$ is unbiased since the random variables are i.i.d and $\psi(x_{i_1}, \dotsc, x_{i_k})$ is a kernel function.

    \item Consider another symmetric unbiased estimator $f$.

    \item By Lemma \ref{lemA}, since $E(f - \psi^{[n]}) = 0$, we know $f - \psi^{[n]} = 0$, i.e. $f = \psi^{[n]}$.
\end{itemize}   . 
\end{proof}
\end{frame}

\begin{frame}{Characterization of ``Best'' Estimator}
 \begin{theorem}
    Suppose that $E = \R$. Let $\theta$ be a regular functional of degree k defined by Equation \ref{unbiased_est} on a set $\mathcal{F}$ of distribution functions containing all distributions having finite support. Let $f$ be an unbiased estimate of $\theta$ based on a sample of size n, so that $f$ satisfies Equation \ref{unbiased}. Then $var(f) \geq var(\psi^{[n]})$ for all $F \in \mathcal{F}$. 
    \end{theorem}

    % \begin{proof}
    % \begin{fontsize}{8.5}{12}
    % \begin{itemize}
    %     \item Define $f^{[n]}(x_1, \dotsc, x_n) = \frac{1}{n!}\sum_{(n)}f(x_{i_1}, \dotsc, x_{i_n})$ where the sum $\sum_{(n)}$ is taken over all permutations $(i_1, \dotsc, i_n)$ of $\{1, \dotsc, n\}$.

    %     \item  $f^{[n]}$ is a symmetric unbiased estimator, and so by Theorem \ref{unique} agrees with $\psi^{[n]}$ on $\R$.
    %     \pause
    %     \item Use the Cauchy-Schwartz Inequality to show $(\psi^{[n]})^2 \leq \frac{1}{n!}\sum_{(n)}f^2(x_{i_1}, \dotsc, x_{i_n})$.

    %     \item Take the expected value of the inequality to show $E((\psi^{[n]})^2) \leq E(f^2(x_{i_1}, \dotsc, x_{i_n}))$.
    %     \pause
    %     \item This implies that $var(f) \geq var(\psi^{[n]})$.
    % \end{itemize}
    % \end{fontsize}
    % \end{proof} 
  \pause   
     \begin{Rmk}
 There is an equivalent theorem for when $\mathcal{F}$ contains distribution functions containing all absolutely continuous distribution functions. 
 \end{Rmk}
\end{frame}

\section{U-statistics}
\begin{frame}{U-statistics}
U-statistics were first coined by Hoeffding (\cite{hoeffding1948}) and are named due to their unbiasedness
\begin{Def}
Statistics that have the form
        \begin{align}\label{u_stat}
        \hat{\theta} = {n \choose k}^{-1} \sum_{(n,k)} \psi(X_{i_1}, \dotsc, X_{i_k})
    \end{align} 
have desirable properties as estimators of regular functionals. These statistics are known as \textit{U-statistics}. 
\end{Def}
\end{frame}
\begin{frame}{Examples}
\begin{ex}\textbf{Sample mean}
Let $\mathcal{F}$ be the set of all distributions whose means exist, which includes sets $\mathcal{F}$ that contain all distributions that have finite support on $\R$. The mean functional is given by \begin{align*}\theta(F) = \int_{-\infty}^{\infty} x dF(x).\end{align*} Using Equation \ref{u_stat}, we have that $k=1$ and $\psi(X_{i_1}) = x_{i_1}$, which gives \begin{align*}\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i.\end{align*}
\end{ex}
% \pause
% \begin{ex}\textbf{Sample variance}
% Let $\mathcal{F}$ be the set of all distributions with finite second moment, i.e. $\mathcal{F} = \{F: \int |x|^2 dF(x)<\infty\}$. The variance functional is given by $\theta(F) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2}(x_1 - x_2)^2 dF(x_1)dF(x_2)$. Here, we have that $k=2$ and $\psi(X_{i_1}, X_{i_2}) = \frac{1}{2}(x_{i_1} - x_{i_2})^2$, which gives $s_n^2 = {n \choose 2}^{-1}\sum_{1 \leq i < j \leq n}\frac{1}{2}(X_i - X_j)^2$.
% \end{ex}
\end{frame}

\begin{frame}{Examples}
\begin{ex}\textbf{Kendall's Tau} Here, we consider two points $P_1$ and $P_2$ on a plane. We say that $P_1$ and $P_2$ are \textit{concordant} if the line joining the points have positive slope, and otherwise, they are \textit{discordant}. Let $\mathcal{F}$ be the set of distribution functions with all absolutely continuous bivariate random vectors. The functional is $\tau = \mathbb{P}(P_1, P_2 \text{ concordant}) - \mathbb{P}(P_1, P_2 \text{ discordant})$. Define the kernel $t$ as 
\begin{fontsize}{10}{12}
\begin{align*}
    t(P_1, P_2) = \begin{cases} 1 & \text{if $P_1$ and $P_2$ are concordant} \\ -1 & \text{if $P_1$ and $P_2$ are discordant} 
    \end{cases} = sgn(X_1 - X_2)(Y_1 - Y_2).
\end{align*}
\end{fontsize}
\noindent Here, we have $k=2$. Using Equation \ref{u_stat}, we have that the estimator of Kendall's tau is $\tau_n = {n \choose 2}\sum_{(n,2)}t(P_i, P_j)$.
\end{ex}    
\end{frame}

\begin{frame}{Conditional Expectation}
We first define the conditional expectation.
\begin{Def}
Given $c = 1, \dotsc, k$, we denote \begin{align}
    \psi_c(x_1, \dotsc, x_c) &= E(\psi(x_1, \dotsc, x_c, X_{c+1}, \dotsc X_k)) \notag \\
    &= E(\psi(X_1, \dotsc, X_k)|X_1 = x_1, \dotsc, X_c = x_c)
\end{align}
as the \textit{conditional expectation}. We also denote the variance of the conditional expectation as 
\begin{align}
    \sigma_c^2 = var(\psi_c(X_1, \dotsc, X_c))
\end{align}
where $\sigma_0^2 = 0$. 
\end{Def}
\end{frame}

\begin{frame}{Conditional Expectation}
\begin{theorem}\label{cond_exp}
The functions $\psi_c$ defined above have the properties
        \begin{itemize}
            \item $\psi_c(x_1, \dotsc, x_c) = E(\psi_d(x_1, \dotsc, x_c, X_{c+1}, \dotsc, X_d)), \text{ for } 1 \leq c <d \leq k$
            \item $E(\psi_c(X_1, \dotsc, X_c)) = E(\psi(X_1, \dotsc, X_k))$
        \end{itemize}
\end{theorem}

\pause
\begin{theorem}\label{cond_exp_prop}
We can express the variance of the conditional expectation as \begin{align}
    \sigma_c^2 = cov(\psi(S_1), \psi(S_2))
\end{align}
where $S_1, S_2$ are two subsets of size $k$ from $\{1, \dotsc, n\}$ with $c$ elements in common. 
\end{theorem}
\end{frame}

\begin{frame}{Conditional Expectation}
\begin{theorem}
We first define $\sigma_{c,d}^2 = cov(\psi_c(X_1, \dotsc, X_c), \phi_d(X_1, \dotsc, X_d)$. Suppose that $c \leq d$ and $k_1 \leq k_2$. If $S_1$ is in $S_{n,k_1}$ and $S_2$ in $S_{n, k_2}$ with $|S_1 \cap S_2| = c$, then 
    \begin{align}
    \sigma_{c,d}^2 = cov(\psi(S_1), \phi(S_2))
    \end{align}
\end{theorem}
\pause 
\begin{Rmk}
 As a consequence, we have $\sigma_{c,c}^2 = \sigma_{c,c+1}^2 = \dotsc = \sigma_{c,k_2}^2$ for $c = 1, \dotsc, k_1$.    
\end{Rmk}
\end{frame}

\begin{frame}{Variance and Covariance of U-statistics}
\begin{theorem}\label{var_u_stat}
Let $U_n$ be a U-statistic with kernel $\psi$ of degree k. Then, 
\begin{align}
    var(U_n) = {n \choose k}^{-1}\sum_{c=1}^k{k \choose c}{n-k \choose k-c} \sigma_c^2
\end{align}
\end{theorem} 
\pause 
Consider two U-statistics $U_n^{(1)}$ and $U_n^{(2)}$ that are based on the same sample $X_1, \dotsc, X_n$, but they have different kernel functions, $\psi$ and $\phi$ of degrees $k_1$ and $k_2$, respectively. Assume that WLOG, $k_1 \leq k_2$. 
    \begin{theorem}
    Let $U_n^{(1)}$ and $U_n^{(2)}$ be defined as above. Then, 
    \begin{align}
        cov(U_n^{(1)}, U_n^{(2)}) = {n \choose k_1}^{-1}\sum_{c=1}^{k_1}{k_2 \choose c}{n-k_2 \choose k_1 - c}\sigma_{c,c}^2
        \end{align}
    \end{theorem}  

\end{frame}

\begin{frame}{H-decomposition}
U-statistics of degree $k$ can be decomposed in terms of a sum of uncorrelated U-statistics of degree $1, 2, \dotsc, k$  (\cite{hoeffding1961strong}). 

\begin{Def}
We have kernels $h^{(1)}, \dotsc, h^{(k)}$ of degrees $1, \dotsc, k$ that are defined recursively:
\begin{align}
    h^{(1)}(x_1) &= \psi_1(x_1) - \theta \notag \\
    &\dotsc \notag \\
    h^{(c)}(x_1, \dotsc, x_c) &= \psi_c(x_1, \dotsc, x_c) - \sum_{j=1}^{c-1}\sum_{(c,j)}h^{(j)}(x_{i_1}, \dotsc, x_{i_j}) - \theta \label{h_recursive}
\end{align}
where $c = 2, \dotsc, k$. 
\end{Def}
\end{frame}

\begin{frame}{H-decomposition}
 \begin{theorem}
For $j = 1, \dotsc, k$, let $H_n^{(j)}$ be the U-statistic based on the kernel $h^{(j)}$, where \begin{align}
H_n^{(j)} = {n \choose j}^{-1} \sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j}). 
\end{align}
Then,
\begin{align}
    U_n  = \theta + \sum_{j=1}^k{k \choose j}H_n^{(j)}
\end{align}
\end{theorem}   
\end{frame}

\begin{frame}{H-decomposition}
\begin{proof}
\begin{itemize}
    \item Let $S_j\{i_1, \dotsc, i_k\}$ denote the sum $\sum_{(n,j)} h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j})$. 
    \item We have $\sum_{(n,k)}S_j\{i_1, \dotsc, i_k\} = {n-j \choose k-j}\sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j})$ 
    \item  ${n-j \choose k-j}{n \choose j} = {n \choose k}{k \choose j} \implies {n \choose k}^{-1}{n-j \choose k-j} = {k \choose j}{n \choose j}^{-1}$
\end{itemize}
\begin{fontsize}{10}{12}
\begin{align*}
    U_n &= {n \choose k}^{-1}\sum_{(n,k)}\psi(x_{i_1}, \dotsc, x_{i_k}) = {n \choose k}^{-1}\sum_{(n,k)}\left[\sum_{j=1}^kS_j\{i_1, \dotsc, i_k\} + \theta \right] \\
    &= \theta + {n \choose k}^{-1}\sum_{j=1}^k{n-j \choose k-j}\sum_{(n,j)}h^{(j)}(x_{\nu_1}, \dotsc, x_{\nu_j}) \\
    &= \theta + {n \choose k}^{-1}\sum_{j=1}^k{k \choose j}H_n^{(j)}
\end{align*}
\end{fontsize}
\end{proof}    
\end{frame}

\begin{frame}{H-decomposition}
\begin{Rmk}
We can also truncate the H-decomposition up to $c$ terms 
\begin{align}
    U_n = \theta + \sum_{j=1}^c{k \choose j} H_n^{(c)} + R_n^{(c)}
\end{align}
where $R_n^{(c)}$ is a U-statistic with kernel $\sum_{j=c+1}^k S_j\{1, \dotsc, k\}$.  
\end{Rmk}
\end{frame}

\begin{frame}{H-decomposition}
\begin{Rmk}
We can represent the functions $h^{(c)}$ in another way. Let $G_x$ denote the distribution function of a single point mass at $x$. 
\begin{fontsize}{6.5}{12}
\begin{align}
    h^{(1)}(x_1) - \psi(x_1) - \theta  &= \int \dotsc \int \psi(x_1, u_2, \dotsc, d_k)\prod_{i=2}^k dF(u_i) - \int \dotsc \int \psi(u_1, u_2, \dotsc, d_k)\prod_{i=1}^k dF(u_i) \notag \\
    &= \int \dotsc \int \psi(u_1, \dotsc, u_k)(dG_{x_1}(u_1) - dF(u_1))\prod_{i=2}^k dF(u_i) \notag \\
    h^{(j)}(x_1, \dotsc, x_j) &= \int \dotsc \int \psi(u_1, \dotsc, u_k) \prod_{i=1}^j(dG_{x_i}(u_i) - dF(u_i))\prod_{j+1}^kdF(u_i) \label{h_func_rep}
\end{align}
\end{fontsize}
\end{Rmk}    
\end{frame}

\begin{frame}{H-decomposition}
\begin{fontsize}{8}{12}
     We denote the $c$-th conditional expectation of $h^{(j)}$ by $h_c^{(j)}$. Then $h^{(j)}$ has these properties: 
    \begin{theorem}
    \begin{itemize}
        \item For $c = 1, \dotsc, j-1$ and $j = 1, \dotsc, k$, $h_c^{(j)}(x_1, \dotsc, x_c) = 0$
        \item $E(h^{(j)}(X_1, \dotsc, X_j)) = 0$
    \end{itemize}
    \end{theorem}
    
   The variances of the conditional expectations of $h^{(j)}$ are all zero, except the j-th. We define 
    \begin{align}
        \delta_j^2 = var(h_j^{(j)}(X_1, \dotsc, X_j)) = var(h^{(j)}(X_1, \dotsc, X_j))
    \end{align}
    Then by the theorem on variance of U-statistics, we have
    \begin{align}
        var(H_n^{(j)}) = {n \choose j}^{-1}\sum_{c=1}^j {j \choose c}{n-j \choose j-c}var(h_c^{(j)}(X_1, \dotsc X_c)) = {n \choose j}^{-1}\delta_j^2
         \end{align}
         \end{fontsize}
    \end{frame} 
    
    \begin{frame}{H-decomposition}
        
        \begin{theorem}
    \begin{itemize}
        \item Let $j <j'$ and let $S_1$ and $S_2$ be a j-subset of $\{1, \dotsc, n\}$ and a j'-subset of $\{1, \dotsc, n\}$, respectively. Then, $cov(h^{(j)}(S_1), h^{(j')}(S_2)) = 0$ and so $cov(H_n^{(j)}, H_n^{(j')}) = 0$
        \item Let $S_1 \neq S_2$ be two distinct j-subsets of $\{1, \dotsc, n\}$. Then, $cov(h^{(j)}(S_1), h^{(j)}(S_2)) = 0$
    \end{itemize}
    \end{theorem}
    \end{frame}
    
    
\begin{frame}{H-decomposition}
    \begin{theorem}
    The variance of $U_n$ is given by 
    \begin{align}
        var(U_n) = \sum_{j=1}^k {k \choose j}^2 {n \choose j}^{-1}\delta_j^2
    \end{align}
    \end{theorem}
    Relations describing $\sigma_{c}^2$ in terms of the $\delta_j^2 $ and vice versa can be derived as follows: from the definition of H-decomposition, we have 
    \begin{align*}
        \phi_c (X_1, \cdots, X_c) = \sum_{j = 1}^c \sum_{c,j} h^{(j)} (X_{i_{1}}, \cdots , X_{i_j}) + \theta
    \end{align*}
    \end{frame}
    
    \begin{frame}{H-decomposition}
    \begin{fontsize}{7}{12}
    Take variance of both sides,  we get 
    \begin{align*}
        \sigma_{c}^2 &= \sum_{j = 1}^c Var\{ \sum_{(c, j)} h^{(j)} (X_{i_{1}}, \cdots , X_{i_j})\} \\
        &= \sum_{j = 1}^c {c \choose j}^2 Var( H_c^{(j)}) \\ 
        &= \sum_{j = 1}^c {c \choose j} \delta_j^2.
    \end{align*}

    \begin{align*}
        \sum_{c=1}^d {d \choose c} (-1)^{d-c} \sigma_{c}^2 &= \sum_{c=1}^d  
       \sum_{j = 1}^c {c \choose j}  {d \choose c} (-1)^{d-c} \delta_j^2 \\
       &= \sum_{j=1}^d \left\{ \sum_{c = 0}^{d - j} {d - j \choose c} (-1)^c \right\} (-1)^{d -j} {d \choose j} \delta_j^2.
    \end{align*}
    The term inside the braces is zero except when $j = d$; hence we get 
    \begin{align*}
        \delta_d^2 = \sum_{c = 1}^d (-1)^{b-c} {d \choose c} \sigma_{c}^2
    \end{align*}
    \end{fontsize}
    \end{frame}
    
    

\section{Central Limit Theorem}
\begin{frame}{CLT for i.i.d case}

\begin{fontsize}{7}{12}  
\begin{theorem}
    Let $\sigma_1^{2} > 0$. Then $n^{\frac{1}{2}}(U_n - \theta)$ is asymptotically normal with mean zero and asymptotic variance $k^2 \sigma_1^2$.
    \end{theorem}
    
    \begin{proof}
      First, by definition of H-decomposition, we have 
      \begin{align*}
          \sqrt{n}(U_n - \theta) &= \sqrt{n} \left( {k \choose 1} H_n^{(1)} + R_n \right) 
          = \sqrt{n} \left( k {n \choose 1}^{-1} \sum_{ (n,1) } h^{(1)} + R_n \right) \\
          &= \frac{\sqrt{n}}{n} k \sum_{i = 1}^{n} h^{(1)}(x_i) + \sqrt{n}R_n 
          = \frac{k}{\sqrt{n}} \sum_{i = 1}^{n} h^{(1)}(x_i) + \sqrt{n}R_n.
          \end{align*}
      Note that $\sum_{i = 1}^{n} h^{(1)}(x_i)$ is i.i.d by assumption, and $$ E (\frac{k}{\sqrt{n}} \sum_{i = 1}^{n} h^{(1)}(x_i)) = \frac{k}{\sqrt{n}} E( \sum_{i = 1}^{n}h^{(1)} (x_i) ) = 0$$
      by Theorem 10 (ii). 
      \end{proof}
      \end{fontsize}
      \end{frame}
      
      \begin{frame}{CLT proof cont. }
      \begin{fontsize}{7}{12}
      \begin{proof}
        Similarly, we have $$E(\sqrt{n}R_n) = \sqrt{n} \sum_{j = 2}^k {k \choose 1} E(H_n^{(j)}) = 0,$$
      thus $E\left( n^{\frac{1}{2}}(U_n - \theta) \right) = 0. $ 
      Next, by orthogonality of $H_n$ and $R_n$,
      \begin{align*}
          Var(\sqrt{n}(U_n - \theta) ) &= Var(\sqrt{n} U_n) + Var(\sqrt{n}R_n) 
          = \frac{k^2}{n} \sum_{i = 1}^n Var(h^{(1)} (x_i) )  + n Var(R_n) \\
          &= \frac{k^2}{n} n \sigma_1^2 + n Var( \sum_{j = 2}^k {k \choose j} H_n^{(j)}) 
          = k^2 \sigma^2 + n \sum_{j = 2}^k {k \choose j}^2 Var(H_n^{(j)}) \\
          &= k^2 \sigma^2 + n \sum_{j = 2}^k {k \choose j}^2 {n \choose j}^{-1} \delta_j^2 
          = k^2 \sigma^2 + \mathcal{O}(n^{-1}).
      \end{align*}
      Hence asymptotically we have $Var(n^{\frac{1}{2}}(U_n - \theta) ) = k^2 \sigma^2$. 
      By Markov's inequality, for all $\epsilon > 0,$ $P(|\sqrt{n}R_n > \epsilon| ) < \frac{Var(\sqrt{n} R_n)}{\epsilon^2} \rightarrow 0$ as $n \rightarrow \infty$.
      By Slutsky's theorem, the asymptotic behavior of $\sqrt{n}(U_n - \theta)$ only depends on that of $ \sum_{i = 1}^{n}h^{(1)} (x_i)$. So, $ \sqrt{n}(U_n - \theta) \xrightarrow{\mathcal{D}} \mathcal{N}(0, k^2 \sigma_1^2 ).$
    \end{proof}
    \end{fontsize}
\end{frame}

    
    \begin{frame}{CLT for U-statistics for Independent r.v.s}
    \begin{fontsize}{6}{12}

    \begin{theorem}
     Let $X_1, X_2, \cdots,$ be independent r.v.s with distribution functions $F_1, F_2, \cdots$. Define for $i = 1,2, \cdots, n$
    \begin{align}
        \phi_{1,i} (x) = {n-1 \choose k-1}^{-1} \sum_{(n-1, k-1)}^{(-i)} \phi_{1, i; i_2, \cdots, i_k}(x)
    \end{align}
    where the sum is taken over all $(k-1)$-subsets $\{i_2, \cdots, i_k\}$ of $\{1,2, \cdots, n\}$ that do not contain $i$, and 
    \begin{align}
        \phi_{1, i; i_2, \cdots, i_k}(x) = E \phi(x, X_{i2}, \cdots, X_{i_k}) - \theta(i, i_2, \cdots, i_k).
    \end{align}
    Suppose that 
    \begin{itemize}
        \item $Var \phi(X_{i_1}, \cdots, X_{i_k} ) < A$ for some constant $A$ and all $i_1, \cdots, i_k$;
        \item For some $\delta > 0, E|\phi_{1, i} (X_i) |^{2 + \delta} < \infty$ for all $i$, and 
        \begin{align}
            \lim_{n \rightarrow \infty} \sum_{i =1}^{n} E|\phi_{1, i} (X_i) |^{2 + \delta} \Big/ (\sum_{i =1}^{n} E|\phi_{1, i} (X_i)^2 |) ^{(2 + \delta)/2} = 0
        \end{align}
       Then $(U_n - E(U_n)) / (Var U_n)^{\frac{1}{2}} \xrightarrow{\mathcal{D}} N(0,1).$ 
    \end{itemize}
    \end{theorem}
    \end{fontsize}
  \end{frame}
  
  \begin{frame}{CLT independent case proof sketch}
      
  \end{frame}

\section{Reverse Martingales}
\begin{frame}{Reverse Martingales}
\begin{Def}
An integrable sequence $\{X_n: n\ge 1\}$ adapted to an increasing sequence of sub $\sigma$-fields $\{F_n: n\ge 0\}$ is a \textit{martingale} if
\begin{align*}
 E(X_{n+1}|\mathcal{F}_n)=X_n \hspace{0.2cm} a.s., \forall n\ge0.
\end{align*}
\end{Def}
\begin{Def}
An integrable sequence $\{X_n: n\ge 1\}$ adapted to a decreasing sequence of sub $\sigma$-fields $\{F_n: n\ge 0\}$ is a \textit{reverse martingale} if
\begin{align*}
 E(X_m|\mathcal{F}_n)=X_n \hspace{0.2cm} a.s., \forall n\ge m.
\end{align*}
\end{Def}
\end{frame}
\begin{frame}{Convergence result for reverse martingales}
\begin{theorem}\label{conv_rm}
Let $\{X_n\}$ be a reverse martingale. Then $X_n$ converges to $E(X_1|\mathcal{F}_{\infty})$ almost surely and in $L_1$. 
\end{theorem}
\end{frame}

\section{U-statistics as martingales}
\begin{frame}{U-statistics as martingales}
\begin{theorem}\label{mat}
Let $U_n$ be a sequence of U-statistics based on a kernel $\psi$ satisfying $E|\psi(X_1, \dotsc, X_k)|<\infty$, and let $\mathcal{F}_n=\sigma\{X_1, \dotsc, X_n\}$. Then $\{{n \choose c} H_n^{(c)}\}_{n=c}^\infty$ is a martingale adapted to the $\mathcal{F}_n$ for $c=1,\dotsc, k$.
\end{theorem}
\begin{fontsize}{9}{12}
\begin{proof}
\begin{itemize}
    \item Check r.v.s $H_n^{(c)}$ are integrable: By
\begin{align*}
h^{(1)}(x_1)&=\psi_1(x_1)-\theta\\
h^{(c)}(x_1,\dotsc,x_c)&=\psi_c(x_1,\dotsc,x_c)-\sum_{j+1}^{c-1}\sum_{(c,j)}h^{(j)}(x_{i_1},\dotsc,x_{i_j})-\theta
\end{align*}
Since  and $\psi_1(x_1)$ is integrable by law of total expectation and the assumption that $E|\psi(X_1, \dotsc, X_k)|<\infty$, then $h^{(1)}(x_1)$ is integerable. Recursively we can get $h^{(j)}(x_{i_1},\dotsc,x_{i_j})$ is integrable and $H_n^{(j)}={n\choose j}h^{(j)}(x_{i_1},\dotsc,x_{i_j})$ is also integrable.
\end{itemize}
\end{proof}
\end{fontsize}
\end{frame}

\begin{frame}{Proof cont'd}
\begin{proof}
\begin{fontsize}{9}{12}
\begin{itemize}
    \item Plug in the definition for $H_{n+1}^{(c)}$:
\begin{align*}
E\{{{n+1}\choose c} H_{n+1}^{(c)}|\mathcal{F}_n\}&=
\sum_{(n+1,c)}E\{h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}\\
&=\sum_{(n,c)}h^{(c)}(X_{i_1},\dotsc,X_{i_c})={n\choose c}H_n^{(c)}
\end{align*}
\end{itemize}
 $E\{h^{(c)}(X_{i_1},\dotsc,X_{i_c})|\mathcal{F}_n\}=h^{(c)}(X_{i_1},\dotsc,X_{i_c})$ provided all the indices $i_j\le n$ for all $j \in \{i,\dotsc, c\}$. If there exist $l$ indices with $i_j\ge n+1$, Since the inherent symmetry property of the kernal function, w.l.o.g we can assume that the first $l$ random variabels are not measurable, then by Theorem 2 (i) for the H-decomposition.
\begin{align*}
E\{h^{(c)}(X_{i_1},\dotsc,X_{i_l},X_{i_{l+1}},\dotsc, X_{i_c})|\mathcal{F}_n\}&=E\{h^{(c)}(X_{i_1},\dotsc,X_{i_l},x_{i_{l+1}},\dotsc, x_{i_c})\}\\&=h^{(c)}_{c-l}(x_{i_1},\dotsc,x_{i_c})=0
\end{align*}
\end{fontsize}
\end{proof}
\end{frame}

\begin{frame}{U-statistics as martingales}
\begin{theorem}
Let $U_n$ be a sequence of U-statistics based on a kernel $\psi$ satisfying $E|\psi(X_1, \dotsc, X_k)|<\infty$. Then $\{U_n\}_{n=k}^\infty$ is a reverse martingale adapted to the $\sigma$-fields $\mathcal{F}_n=\sigma\{U_n, U_{n+1,\dotsc}\}$.
\end{theorem}
\begin{proof}
\begin{fontsize}{9}{12}
By the inherent symmetry of the kernel $\psi$ and that samples $X_1, \dotsc, X_k$ are i.i.d, write:
\begin{align*}
    E(\psi (X_{i_1}, \dotsc, X_{i_k})|\mathcal{F}_n)=E(\psi (X_1, \dotsc, X_k)|\mathcal{F}_n)
\end{align*}
for every subset $\{i_1, \dotsc, i_k\}$ of $\{1,2,\dotsc,n\}$ and hence 
\begin{align*}
    U_n=E(U_n|\mathcal{F}_n)&={n\choose k}^{-1}\sum_{(n,k)}E(\psi(X_{i_1}, \dotsc,X_{i_k})|\mathcal{F}_n)
    =E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_n).
\end{align*}
\end{fontsize}
\end{proof}
\end{frame}

\begin{frame}{Proof Cont'd}
\begin{fontsize}{9}{12}
\begin{proof}
\begin{itemize}
    \item Check the integrability of $U_n$
\begin{align*}
E(|U_n|)&=E(|E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_n)|)\le E(E(|\psi(X_1,\dotsc,X_k)||\mathcal{F}_n))\\
&=E(|\psi(X_1,\dotsc, X_k)|)<\infty
\end{align*}
    \item Let $n\ge m$. Then since $\mathcal{F}_n \subseteq \mathcal{F}_m$,
    \begin{align*}
    E(U_m|\mathcal{F}_n)&=E\{E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_m)|\mathcal{F}_n\}\\
    &=E\{\psi(X_1,\dotsc,X_k)|\mathcal{F}_n\}
    =U_n.
\end{align*}
\end{itemize}
\end{proof}
\end{fontsize}
\end{frame}

\section{Strong Law of Large Numbers}
\begin{frame}{SLLN for U-statistics}
\begin{theorem}
Suppose $E|\psi(X_1, \dotsc,X_k)|<\infty$. Then $U_n$ converges a.s. to $\theta$.
\end{theorem}
Outline of the proof:
\begin{itemize}
    \item Prove $E(\lim_n U_n)=\theta$
    \item Define $\mathcal{G}_n=\sigma(X_n,X_{n+1},\dotsc)$ and $\mathcal{G}_\infty=\bigcap_{n=k}^\infty\mathcal{G}_n$. Prove that $\lim U_n$ is $\mathcal{G}_\infty$-measurable, then by the Kolmogorov Zero-One law, $\lim U_n$ is a constant which equals to $\theta$ almost surely.
\end{itemize}
\end{frame}

\begin{frame}{Proof for SLLN for U-statistics}
Proof for step 1:
\begin{proof}
Since $U_n$ is a reverse martingale adapted to the $\sigma$-field $\mathcal{F}_n$, we have by the convergence property of reverse martingale:
\begin{align*}
\lim_n U_n = \lim_n E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_n)=E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_\infty)    
\end{align*}
where $\mathcal{F}_\infty$ is the $\sigma$-field $\bigcap_{n=k}^\infty \mathcal{F}_n$. Hence
\begin{align*}
E(\lim_n U_n)=E\{E(\psi(X_1,\dotsc,X_k)|\mathcal{F}_\infty)\}=E\psi(X_1, \dotsc,X_k)=\theta
\end{align*}
\end{proof}
\end{frame}

\begin{frame}{Proof for SLLN for U-statistics Cont'd}
Proof for step 2:
\begin{fontsize}{6}{12}
\begin{proof}
To prove this, we need to show that $\lim U_n$ is $\mathcal{G}_m$-measurable for some arbitrary $m>k$. Write
\begin{align*}
U_n={n\choose k}^{-1}\underbrace{S_n}_{\mathcal{G}_m\text{-measurable}}+{n\choose k}^{-1}\underbrace{T_n}_{\text{not } \mathcal{G}_m\text{-measurable}}
\end{align*}
where $S_n$ and $T_n$ are the sums of all terms $\psi(X_{i_1},\dotsc,X_{i_k})$ for which:
\begin{itemize}
    \item $S_n$: all indices of $X_{i_j}$ are $\ge m$
    \item $T_n$: some indices of $X_{i_j}$ are $< m$
\end{itemize}
\begin{align*}
\lim_n {n\choose k}^{-1} T_n=\lim_n {n\choose k}^{-1} \sum_{p=1}^{k-1}T_n^{(p)}=\sum_{p=1}^{k-1}\lim_n\underbrace{ {{n-m+1}\choose p}{n\choose k}^{-1}}_{\mathcal{O}(n^{-1})}\underbrace{{{n-m+1}\choose p}^{-1}T_n^{(p)}}_{\xrightarrow{a.s.}\text{integrable r.v.}}=0
\end{align*}
\begin{itemize}
    \item $T_n^{(p)}$: $\psi$ summed over all sets having exactly $p$ indices $\ge m$ ($\mathcal{G}_m$-measurable). Since need at least 1 non-measurable variable, $k-p\ge1$.
    \item ${{n-m+1}\choose p}^{-1}T_n^{(p)}$: reverse martingale. By convergence theorem, convergent a.s. to integrable r.v.
\end{itemize}
\end{proof}
\end{fontsize}
\end{frame}

\begin{frame}{References}
\printbibliography
\end{frame}

\end{document}